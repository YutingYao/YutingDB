ğŸ˜ğŸ˜è¿™ä¸ªæ–‡æ¡£ä¸»è¦ç”¨æ¥è®°å½•è¸©è¿‡çš„å‘ï¼ŒæˆåŠŸç‰ˆå¯å‚è€ƒæœ¬ç›®å½•ä¸‹çš„`å…¶ä»–markdownæ–‡æ¡£`ã€‚

<!-- vscode-markdown-toc -->
* [1.1. ä¸‰æ­¥èµ°](#)
* [1.2. å®‰è£…è¾“å…¥æ³•ibus éœ€è¦é‡å¯ï¼ˆä½†è¿™ä¸€æ­¥,è²Œä¼¼ä¸éœ€è¦ï¼‰](#ibus)
* [1.3. å®‰è£…è¿œç¨‹æ§åˆ¶ï¼ˆä½†è¿™ä¸€æ­¥,ç›®å‰æ²¡æœ‰æˆåŠŸï¼‰](#-1)
* [2.1. dockerç¯å¢ƒ](#docker)
* [ hadoop](#hadoop)
	* [ é™é»˜è­¦å‘Šï¼ˆç”±äºä½¿ç”¨äº†32ä½Hadoopæ„å»ºå’Œ64ä½æ“ä½œç³»ç»Ÿï¼‰](#32Hadoop64)
* [ scala](#scala)
* [ spark](#spark)
	* [2.6.1. ä¸‹è½½Sparkï¼Œè§£åŒ…å¹¶æˆäºˆpiæ‰€æœ‰æƒ](#Sparkpi)
	* [ é…ç½®Sparkç¯å¢ƒå˜é‡](#Spark)
	* [ é…ç½®spark-env.sh](#spark-env.sh)
	* [ é…ç½®slaves](#slaves)
	* [2.6.5. å¯åŠ¨Sparké›†ç¾¤](#Spark-1)
		* [2.6.5.1. å¯åŠ¨Hadoopé›†ç¾¤](#Hadoop)
		* [ å¯åŠ¨Sparké›†ç¾¤](#Spark-1)
		* [ å…³é—­Sparké›†ç¾¤](#Spark-1)
		* [2.6.5.4. å¯åŠ¨binç›®å½•ä¸‹çš„spark-shell](#binspark-shell)
	* [2.6.6. ä¸ºäº†æ–¹ä¾¿å¯ä»¥ä¿®æ”¹Bashç¯å¢ƒå˜é‡é…ç½®](#Bash)
	* [2.6.7. é…ç½®Sparkä½œä¸šç›‘æ§](#Spark-1)
* [2.7. 2.5 pyspark](#pyspark)
	* [2.7.1. ä½¿ç”¨Spark](#Spark-1)
	* [2.7.2. åœ¨Sparkä¸­é‡‡ç”¨æœ¬åœ°æ¨¡å¼å¯åŠ¨pyspark](#Sparkpyspark)
	* [2.7.3. pysparkç‹¬ç«‹åº”ç”¨ç¨‹åºç¼–ç¨‹](#pyspark-1)
	* [2.7.4. Sparkåº”ç”¨ç¨‹åºåœ¨é›†ç¾¤ä¸­è¿è¡Œ](#Spark-1)
		* [2.7.4.1. å¯åŠ¨Hadoopé›†ç¾¤](#Hadoop-1)
		* [2.7.4.2. Hadoop YARNç®¡ç†å™¨](#HadoopYARN)
* [2.8. geospark](#geospark)
	* [2.8.1. geosparkéƒ¨ç½²](#geospark-1)
	* [2.8.2. geosparkç¤ºä¾‹](#geospark-1)
	* [2.8.3. åˆ›å»ºSpatialRDD(SRDD)](#SpatialRDDSRDD)
	* [2.8.4. ç©ºé—´èŒƒå›´æŸ¥è¯¢(Spatial Range Query)](#SpatialRangeQuery)
* [2.9. kafka](#kafka)
	* [2.9.1. Ubuntu ç³»ç»Ÿå®‰è£…Kafka](#UbuntuKafka)
	* [2.9.2. å®‰è£…æˆåŠŸäº†Kafka](#Kafka)
	* [2.9.3. Sparkå‡†å¤‡å·¥ä½œï¼ˆjaræ–‡ä»¶ï¼‰](#Sparkjar)
	* [2.9.4. ç¼–å†™Sparkç¨‹åºä½¿ç”¨Kafkaæ•°æ®æº](#SparkKafka)
* [2.10. flink](#flink)
* [2.11. PostgreSQL](#PostgreSQL)
* [2.12. MongoDB](#MongoDB)
	* [2.12.1. Mongo Spark Connector è¿æ¥å™¨](#MongoSparkConnector)
		* [2.12.1.1. æ¡ˆä¾‹](#-1)
		* [2.12.1.2. è¿ä»·ç³»ç»Ÿçš„æ¶æ„å›¾](#-1)
	* [2.12.2. Spark ä»»åŠ¡å…¥å£ç¨‹åº](#Spark-1)
	* [2.12.3. Spark ï¼‹ MongoDBæ¼”ç¤º](#SparkMongoDB)
* [2.13. å®‰è£…SBT](#SBT)
	* [2.13.1. Linuxä¸­å®‰è£…SBT](#LinuxSBT)
	* [2.13.2. Sparkå¿«é€Ÿå…¥é—¨ä¹‹SBTå®‰è£…](#SparkSBT)
* [3.1. åœ¨localæ¨¡å¼ä¸‹è¿è¡Œ](#local)
* [3.2. åœ¨remoteæ¨¡å¼ä¸‹è¿è¡Œ](#remote)
* [3.3. yarnæ¨¡å¼ä¸‹çš„è¿è¡Œ](#yarn)
* [3.4. inline configuration](#inlineconfiguration)
* [3.5. hive](#hive)
* [3.6. SQL](#SQL)
* [3.7. Streaming](#Streaming)
* [3.8. kafka](#kafka-1)
* [3.9. python](#python)
* [3.10. spark](#spark-1)
* [3.11. flink - Python env - Conda](#flink-Pythonenv-Conda)
	* [3.11.1. å‡†å¤‡å·¥ä½œ](#-1)
	* [3.11.2. æ­å»º PyFlink ç¯å¢ƒ](#PyFlink)
		* [3.11.2.1. Step 1. åˆ¶ä½œ **JobManager** ä¸Šçš„ **PyFlink Conda** ç¯å¢ƒ](#Step1.JobManagerPyFlinkConda)
		* [3.11.2.2. Step 2. åˆ¶ä½œ TaskManager ä¸Šçš„ PyFlink Conda ç¯å¢ƒ](#Step2.TaskManagerPyFlinkConda)
		* [3.11.2.3. Step 3. åœ¨ PyFlink ä¸­ä½¿ç”¨ Conda ç¯å¢ƒ](#Step3.PyFlinkConda)
* [3.12. Apache Sedona](#ApacheSedona)
* [3.13. oracle (è²Œä¼¼ä¸å¤ªå¸¸ç”¨)](#oracle)
* [3.14. ç®€å•ä»‹ç»oracle](#oracle-1)
	* [3.14.1. è¿æ¥Oracleæ•°æ®åº“](#Oracle)

<!-- vscode-markdown-toc-config
	numbering=false
	autoSave=true
	/vscode-markdown-toc-config -->
<!-- /vscode-markdown-toc -->

Project to Design a Hadoop/Spark [Raspberry Pi 4 Cluster](https://github.com/YutingYao/pi-cluster) for Distributed Computing.

An efficient quick-start tool to build [a Raspberry Pi (or Debian-based) Cluster](https://github.com/YutingYao/RaspPi-Cluster) with popular ecosystem like Hadoop, Spark

Repo for instructions on setting up a micro compute cluster with the [NVidia Jetson Nano boards](https://github.com/YutingYao/JetsonCluster) and potentially Ansible playbooks for configuration and setup.

Setting up a [K3s Kubernetes](https://github.com/YutingYao/jetsonnano-k3s-gpu) cluster on my Nvidia Jetson Nano

Cluster made out of [Nvidia Jetson Nano's](https://github.com/YutingYao/NanoCluster)

# 1. çƒ§å½•ç³»ç»Ÿ

## <a name=''></a>1.1. ä¸‰æ­¥èµ°

1. ä¸‹è½½æ ‘è“æ´¾ubuntué•œåƒ-[Ubuntu Desktop 21.04](https://ubuntu.com/download/raspberry-pi/thank-you?version=21.04&architecture=desktop-arm64+raspi)ï¼Œubuntué•œåƒä½¿ç”¨desktopç‰ˆæœ¬
2. [SDå¡æ ¼å¼åŒ–](https://www.sdcard.org/downloads/formatter/sd-memory-card-formatter-for-windows-download/)
3. [çƒ§å½•ç³»ç»Ÿ](https://www.balena.io/etcher/)

è®¾ç½®language

å°†terminalå’Œtextæ”¾åˆ°æ¡Œé¢ä¸Š

```sh
sudo apt install vim
```

## <a name='ibus'></a>1.2. å®‰è£…è¾“å…¥æ³•ibus éœ€è¦é‡å¯ï¼ˆä½†è¿™ä¸€æ­¥,è²Œä¼¼ä¸éœ€è¦ï¼‰

```sh
#ctrl+alt+tè¿›å…¥ç»ˆç«¯ï¼Œè¾“å…¥ibus
ibus                              #æ£€æµ‹iubs
sudo apt-get install ibus-pinyin  #å®‰è£…è¾“å…¥æ³•
ibus-setup     #æ·»åŠ è¾“å…¥æ³•ï¼ˆpinyinï¼‰
ibus restart   #é‡å¯ibus
```

## <a name='-1'></a>1.3. å®‰è£…è¿œç¨‹æ§åˆ¶ï¼ˆä½†è¿™ä¸€æ­¥,ç›®å‰æ²¡æœ‰æˆåŠŸï¼‰(å°è¯•nç§æ–¹æ³•æœ€ç»ˆæ”¾å¼ƒ)

```sh
sudo apt-get install tightvncserver
sudo tightvncserver

sudo apt install vino
sudo apt-get install dconf-editor
```

å¼€æœºè‡ªå¯

```sh
mkdir -p ~/.config/autostart
cp /usr/share/applications/vino-server.desktop ~/.config/autostart/.
cd /usr/lib/systemd/user/graphical-session.target.wants
sudo ln -s ../vino-server.service ./.
```

é…ç½®æœåŠ¡å™¨

```sh
gsettings set org.gnome.Vino prompt-enabled false
gsettings set org.gnome.Vino require-encryption false
Set a password to access the VNC server
```

è®¾ç½®ç™»å½•å¯†ç 

```sh
gsettings set org.gnome.Vino authentication-methods "['vnc']"
gsettings set org.gnome.Vino vnc-password $(echo -n 'thepassword'|base64)
Reboot the system so that the settings take effect
sudo reboot
```


### å†è¯•ä¸€éï¼š

ubuntu20.04çš„ç³»ç»Ÿä¸å¤ªå¥½æ

```sh
# å®‰è£…å¯èƒ½éœ€è¦ç”¨åˆ°çš„å†…å®¹
sudo apt install tightvncserver gnome-panel gnome-settings-daemon metacity nautilus gnome-terminal gnome-session-flashback gdm3
# ç¬¬ä¸€æ¬¡ä¼šæç¤ºè¾“å…¥å¯†ç ã€‚
# è¿™é‡Œå¯åŠ¨çª—å£æ˜¯ä¸ºäº†è‡ªåŠ¨ç”Ÿæˆä¸€ä»½é…ç½®æ–‡ä»¶
tightvncserver :0 -geometry 1280x720 -depth 24 -dpi 96
# å…ˆæ€æ‰çª—å£
tightvncserver -kill :0
```

ä¿®æ”¹æ–‡ä»¶

æ­¤æ–‡ä»¶çš„æƒé™åº”è¯¥æ˜¯775

```sh
vncserver -kill :1
mv ~/.vnc/xstartup ~/.vnc/xstartup.bak
sudo vim ~/.vnc/xstartup
```

```sh
sudo vim /home/yaoyuting03/.vnc/xstartup
```

```sh
sudo vim ~/.vnc/xstartup
```

æ·»åŠ å†…å®¹:

```sh
#!/bin/bash
xrdb $HOME/.Xresources
startxfce4 &
```

```sh
#!/bin/sh

xrdb $HOME/.Xresources
xsetroot -solid grey
#x-terminal-emulator -geometry 80x24+10+10 -ls -title "$VNCDESKTOP Desktop" &
#x-window-manager &

# Fix to make GNOME work
export XKL_XMODMAP_DISABLE=1
/etc/X11/Xsession

# ä¸‹é¢è¿™ä¸€æ®µæ˜¯æˆ‘åŠ ä¸Šå»çš„
unset SESSION_MANAGER
unset DBUS_SESSION_BUS_ADDRESS
export XKL_XMODMAP_DISABLE=1
export XDG_CURRENT_DESKTOP="GNOME-Flashback:GNOME"
export XDG_MENU_PREFIX="gnome-flashback-"
gnome-session --session=gnome-flashback-metacity --disable-acceleration-check &
```

æ·»åŠ 

```sh
unset DBUS_SESSION_BUS_ADDRESS
```

ä¿®æ”¹

```sh
# /etc/X11/XSession
# æ”¹æˆè¿™ä¸ª
mate-session
```

è¾“å…¥

```sh
#!/bin/sh
unset SESSION_MANAGER
unset DBUS_SESSION_BUS_ADDRESS
startxfce4 &
```

ä½¿å…¶å¯æ‰§è¡Œ:

```sh
chmod +x ~/.vnc/xstartup
```


æ‰§è¡Œ

```sh
sudo chmod +x ~/.vnc/xstartup
vncserver 
```


æœåŠ¡å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯åŠ¨æˆ–åœæ­¢:

ç»ˆæ­¢å½“å‰æ­£åœ¨è¿è¡Œçš„æœåŠ¡å™¨ï¼š

```sh
vncserver -kill :1
```

```sh
sudo service vncserver@1 start/stop
```

```sh
tightvncserver
```

å¯åŠ¨ä¸€ä¸ªæ–°çš„æœåŠ¡å™¨

```sh
# å†æ¬¡å¯åŠ¨çª—å£
tightvncserver :0 -geometry 1280x720 -depth 24 -dpi 96
```

### å¯èƒ½éœ€è¦å®‰è£… xfce4

```sh
sudo apt install xfce4 xfce4-goodies
```

# 2. å®‰è£…å¤§æ•°æ®åˆ†æè½¯ä»¶

[å¤§æ•°æ®æ¶æ„](http://dblab.xmu.edu.cn/blog/988-2/)è¯·å‚è€ƒè¿™ä¸ªé“¾æ¥ã€‚

## <a name='docker'></a>2.1. dockerç¯å¢ƒ

å®‰è£…docker

æ›´æ–°åŒ…ç´¢å¼•å¹¶å®‰è£…å°è£…ï¼Œä»¥ä¾¿åœ¨ HTTPS ä¸Šä½¿ç”¨å­˜å‚¨åº“

```sh
sudo apt-get update
 sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg \
    lsb-release
```

æ·»åŠ å¤šå…‹çš„å®˜æ–¹ GPG å¯†é’¥ï¼š

```sh
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
```

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®¾ç½®ç¨³å®šå­˜å‚¨åº“ã€‚è¦æ·»åŠ å¤œé—´æˆ–æµ‹è¯•å­˜å‚¨åº“ï¼Œåœ¨ä¸‹é¢çš„å‘½ä»¤ä¸­çš„å•è¯åæ·»åŠ å•è¯æˆ–ï¼ˆæˆ–ä¸¤è€…å…¼æœ‰ï¼‰ã€‚

```sh
 echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
```

æ›´æ–°aptåŒ…ç´¢å¼•ï¼Œå®‰è£…Docker Engineå’Œcontaineræœ€æ–°ç‰ˆæœ¬ï¼Œæˆ–è€…ç»§ç»­ä¸‹ä¸€æ­¥å®‰è£…ç‰¹å®šç‰ˆæœ¬:

```sh
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io
```

é€šè¿‡è¿è¡Œhello-worldé•œåƒæ¥éªŒè¯Dockerå¼•æ“æ˜¯å¦æ­£ç¡®å®‰è£…ã€‚

```sh
sudo docker run hello-world
```

## <a name='hadoop'></a> hadoop

### <a name='32Hadoop64'></a> é™é»˜è­¦å‘Šï¼ˆç”±äºä½¿ç”¨äº†32ä½Hadoopæ„å»ºå’Œ64ä½æ“ä½œç³»ç»Ÿï¼‰

ä¿®æ”¹Hadoopç¯å¢ƒé…ç½®:

```sh
sudo vim /opt/hadoop/etc/hadoop/hadoop-env.sh
```

æŠŠè¿™ä¸ªï¼š

```sh
# export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true"
```

æ”¹å˜ä¸ºè¿™ä¸ªï¼š

```sh
export HADOOP_OPTS="-XX:-PrintWarnings â€“Djava.net.preferIPv4Stack=true"
```

ç°åœ¨åœ¨~/.bashrcä¸­ï¼Œæ·»åŠ åˆ°åº•éƒ¨ï¼š

```sh
sudo vim ~/.bashrc
```

```sh
export HADOOP_HOME_WARN_SUPPRESS=1
export HADOOP_ROOT_LOGGER="WARN,DRFA" 
```

æ¥æº~/.bashrcï¼š

```sh
source ~/.bashrc
```

å°†.bashrcå¤åˆ¶åˆ°ç¾¤é›†ä¸­çš„å…¶ä»–èŠ‚ç‚¹ï¼š

```sh
clusterscp ~/.bashrc
```

åˆ›å»ºHadoopç¾¤é›†ç›®å½•ï¼ˆå¤šèŠ‚ç‚¹è®¾ç½®ï¼‰ã€‚

```sh
clustercmd sudo mkdir -p /opt/hadoop_tmp/hdfs
clustercmd sudo chown pi:pi â€“R /opt/hadoop_tmp
clustercmd sudo mkdir -p /opt/hadoop
clustercmd sudo chown pi:pi /opt/hadoop
```

å°†Hadoopæ–‡ä»¶å¤åˆ¶åˆ°å…¶ä»–èŠ‚ç‚¹ã€‚

```sh
for pi in $(otherpis); do rsync -avxP $HADOOP_HOME $pi:/opt; done
```

éªŒè¯æ˜¯å¦åœ¨å…¶ä»–èŠ‚ç‚¹ä¸Šå®‰è£…ï¼š

```sh
clustercmd hadoop version | grep Hadoop
Hadoop 3.2.1
Hadoop 3.2.1
Hadoop 3.2.1
Hadoop 3.2.1
```

ä¿®æ”¹ç”¨äºç¾¤é›†è®¾ç½®çš„Hadoopé…ç½®æ–‡ä»¶ã€‚

```sh
sudo vim /opt/hadoop/etc/hadoop/core-site.xml
```

ä¿®æ”¹æ–‡ä»¶ç»“å°¾ä¸ºï¼š

```xml
åŸæ¥çš„ï¼š
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://pi1:9000</value>
  </property>
</configuration>
ç°åœ¨çš„ï¼š
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://pi1:9000</value>
  </property>
</configuration>
```

```sh
sudo vim /opt/hadoop/etc/hadoop/hdfs-site.xml
```

ä¿®æ”¹æ–‡ä»¶ç»“å°¾ä¸ºï¼š

```xml
åŸæ¥çš„ï¼š
<configuration>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///opt/hadoop_tmp/hdfs/datanode</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///opt/hadoop_tmp/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration> 
ç°åœ¨çš„ï¼š
<configuration>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/opt/hadoop_tmp/hdfs/datanode</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/opt/hadoop_tmp/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>4</value>
  </property>
</configuration> 
```

```sh
sudo vim /opt/hadoop/etc/hadoop/mapred-site.xml
```

ä¿®æ”¹æ–‡ä»¶ç»“å°¾ä¸ºï¼š

```xml
åŸæ¥çš„ï¼š
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
ç°åœ¨åœ¨ï¼š
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
      <value>yarn</value>
  </property>
  <property>
    <name>yarn.app.mapreduce.am.env</name>
      <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
  </property>
  <property>
    <name>mapreduce.map.env</name>
      <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
  </property>
  <property>
    <name>mapreduce.reduce.env</name>
      <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
  </property>
  <property>
    <name>yarn.app.mapreduce.am.resource.mb</name>
      <value>512</value>
  </property>
  <property>
    <name>mapreduce.map.memory.mb</name>
      <value>256</value>
  </property>
  <property>
    <name>mapreduce.reduce.memory.mb</name>
      <value>256</value>
  </property>
</configuration>
```

```sh
sudo vim /opt/hadoop/etc/hadoop/yarn-site.xml
```

ä¿®æ”¹æ–‡ä»¶ç»“å°¾ä¸ºï¼š

```xml
åŸæ¥çš„ï¼š
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>  
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
</configuration> 

ç°åœ¨çš„ï¼š
<configuration>
  <property>
    <name>yarn.acl.enable</name>
    <value>0</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
      <value>pi1</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
      <value>1536</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
      <value>1536</value>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
      <value>128</value>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
      <value>false</value>
  </property>
</configuration> 
```

æ¸…ç†datanodeå’Œnamenodeç›®å½•ã€‚

```sh
clustercmd rm -rf /opt/hadoop_tmp/hdfs/datanode/*
clustercmd rm -rf /opt/hadoop_tmp/hdfs/namenode/*
```

åˆ›å»º/ç¼–è¾‘ master amd worker files.

```sh
cd $HADOOP_HOME/etc/hadoop
vim master
```

åœ¨æ–‡ä»¶ä¸­æ·»åŠ ä¸€è¡Œ:

```sh
pi1
```

```sh
vim workers
```

å°†å…¶ä»–piä¸»æœºåæ·»åŠ åˆ°æ–‡ä»¶ï¼š

```sh
pi2
pi3
pi4
```

ç¼–è¾‘ä¸»æœºæ–‡ä»¶ã€‚

```sh
sudo vim /etc/hosts
```

åˆ é™¤è¯¥è¡Œï¼ˆæ‰€æœ‰èŠ‚ç‚¹å°†å…·æœ‰ç›¸åŒçš„ä¸»æœºé…ç½®ï¼‰ï¼š

```sh
127.0.1.1 pi1
```

å°†æ›´æ–°åçš„æ–‡ä»¶å¤åˆ¶åˆ°å…¶ä»–é›†ç¾¤èŠ‚ç‚¹:

```sh
clusterscp /etc/hosts
```

ç°åœ¨é‡æ–°å¯åŠ¨é›†ç¾¤:

```sh
clusterreboot
```

æ ¼å¼åŒ–å¹¶å¯åŠ¨å¤šèŠ‚ç‚¹é›†ç¾¤ã€‚

```sh
hdfs namenode -format -force
start-dfs.sh && start-yarn.sh
```

ç°åœ¨ï¼Œç”±äºæˆ‘ä»¬å·²ç»åœ¨å¤šèŠ‚ç‚¹é›†ç¾¤ä¸Šé…ç½®äº†Hadoopï¼Œæ‰€ä»¥å½“æˆ‘ä»¬åœ¨ä¸»èŠ‚ç‚¹ï¼ˆpi1ï¼‰ä¸Šä½¿ç”¨jpsæ—¶ï¼Œå°†åªè¿è¡Œä»¥ä¸‹è¿›ç¨‹ï¼š

1. Namenode
2. SecondaryNamenode
3. ResourceManager
4. jps

ä¸‹é¢çš„å†…å®¹å·²ç»å¸è½½åˆ°datanodeï¼Œå¦‚æœæ‚¨sshè¿›å…¥å¹¶æ‰§è¡Œjpsï¼Œæ‚¨å°†çœ‹åˆ°:

1. Datanode
2. NodeManager
3. jps



## <a name='spark'></a> spark

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Spark SQLæ¥æ‰§è¡Œå¸¸è§„åˆ†æï¼Œ

Spark Streaming æ¥æ¥åšæµæ•°æ®å¤„ç†ï¼Œ

ä»¥åŠç”¨Mlibæ¥æ‰§è¡Œæœºå™¨å­¦ä¹ ç­‰ã€‚

Javaï¼Œpythonï¼ŒscalaåŠRè¯­è¨€çš„æ”¯æŒä¹Ÿæ˜¯å…¶é€šç”¨æ€§çš„è¡¨ç°ä¹‹ä¸€ã€‚

å®˜æ–¹çš„æ•°æ®è¡¨æ˜ï¼šå®ƒå¯ä»¥æ¯”ä¼ ç»Ÿçš„MapReduceå¿«ä¸Š100å€ã€‚

[Spark SQL](https://spark.apache.org/sql/)æ˜¯Apache Sparkçš„æ¨¡å—ï¼Œç”¨äºå¤„ç†ç»“æ„åŒ–æ•°æ®ã€‚

[MLlib](https://spark.apache.org/mllib/)æ˜¯Apache Sparkçš„å¯æ‰©å±•æœºå™¨å­¦ä¹ åº“ã€‚

[Spark Streaming](https://spark.apache.org/streaming/)ä½¿æ„å»ºå¯ä¼¸ç¼©çš„å®¹é”™æµåº”ç”¨ç¨‹åºå˜å¾—å®¹æ˜“ã€‚

[GraphX](https://spark.apache.org/graphx/)æ˜¯Apache Sparkç”¨äºå›¾å½¢å’Œå›¾å½¢å¹¶è¡Œè®¡ç®—çš„APIã€‚

[Apache Sparkç¤ºä¾‹](https://spark.apache.org/examples.html)

Apache Spark on [Google Colaboratory](https://mikestaszel.com/2018/03/07/apache-spark-on-google-colaboratory/)

ä½¿ç”¨ [Google Colaboratory](https://medium.com/@chiayinchen/%E4%BD%BF%E7%94%A8-google-colaboratory-%E8%B7%91-pyspark-625a07c75000) è·‘ PySpark

å¦‚ä½•åœ¨3åˆ†é’Ÿå†…å®‰è£…PySparkå’Œ[Jupyterç¬”è®°æœ¬](https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes)

ä½¿ç”¨[spark submit](https://spark.apache.org/docs/latest/submitting-applications.html)å¯åŠ¨åº”ç”¨ç¨‹åº

[spark æ¡ˆä¾‹](https://github.com/YutingYao/spark)




## <a name='geospark'></a>2.8. geospark

[ç³»åˆ—æ•™ç¨‹](https://www.jianshu.com/nb/37398936)

GeoSparkæ˜¯åŸºäºSparkä¹‹ä¸Šçš„åˆ†å¸ƒå¼ç¾¤é›†è®¡ç®—ç³»ç»Ÿã€‚

GeoSparkæ‰©å±•äº†Spark Coreå’ŒSparkSQLå¹¶æå‡ºäº†ç©ºé—´å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼ˆSpatial Resilient Distributed Datasets (SRDDs)ï¼‰åŒæ—¶æä¾›äº†å¯è§†åŒ–ç»„ä»¶ã€‚

ç®€è€Œè¨€ä¹‹å°±æ˜¯å¯ä»¥åˆ©ç”¨å®ƒåœ¨Sparkä¸Šåšç©ºé—´è¿ç®—ã€‚

èƒ½å¤ŸåŸºäºç»çº¬åº¦ç­‰ä¿¡æ¯åˆ›å»º

* ç‚¹ï¼ˆPointï¼‰
* çº¿(LineStringï¼‰
* é¢(Polygon)ã€‚

å¹¶æä¾›äº†å‡ ç§ç©ºé—´æŸ¥è¯¢:

* ç©ºé—´ä¸´è¿‘æŸ¥è¯¢(Spatial KNN Query)
* ç©ºé—´èŒƒå›´æŸ¥è¯¢( Spatial Range Query)
* ç©ºé—´è¿æ¥æŸ¥è¯¢(Spatial Join Query)
* è·ç¦»è¿æ¥æŸ¥è¯¢(Distance Join Query)

[Spatial RDD](https://blog.csdn.net/SUDDEV/article/details/104261704)

å¯¹åº”çš„å‡ ä¸ªç±»ä¸ºï¼š

* åæ ‡ï¼šCoordinate
* ç‚¹ï¼šPointã€MultiPoint
* çº¿ï¼šLineStringã€MultiLineStringï¼ˆå¤šæ¡çº¿ï¼‰ã€LinearRing(ç¯çº¿ï¼‰
* é¢ï¼šPolygonã€MultiPolygon
* é›†åˆï¼šGeometryCollection

### <a name='geospark-1'></a>2.8.1. geosparkéƒ¨ç½²

ç¯å¢ƒå‡†å¤‡

* JDK 1.8
* Scala 2.11.x

```xml
<properties>
    <scala.version>2.11.8</scala.version>
    <spark.version>2.3.4</spark.version>
    <scala.binary.version>2.11</scala.binary.version>
    <geospark.version>1.3.0</geospark.version>
</properties>

<dependencies>
    <dependency>
      <groupId>org.scala-lang</groupId>
      <artifactId>scala-library</artifactId>
      <version>${scala.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_${scala.binary.version}</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <dependency>
      <groupId>org.datasyslab</groupId>
      <artifactId>geospark</artifactId>
      <version>${geospark.version}</version>
    </dependency>

    <dependency>
      <groupId>org.datasyslab</groupId>
      <artifactId>geospark-sql_2.3</artifactId>
      <version>${geospark.version}</version>
    </dependency>
  </dependencies>
```

### <a name='geospark-1'></a>2.8.2. geosparkç¤ºä¾‹

å°é²œï¼šæ–°å»ºä¸€ä¸ªCSVæ–‡ä»¶checkin.csvï¼š

```c
-88.175933,32.360763,gas
-88.388954,32.357073,bar
-88.221102,32.35078,restaurant
```

Code:

```js
package com.suddev.bigdata.core

import org.apache.spark.serializer.KryoSerializer
import org.apache.spark.{SparkConf, SparkContext}
import org.datasyslab.geospark.enums.FileDataSplitter
import org.datasyslab.geospark.serde.GeoSparkKryoRegistrator
import org.datasyslab.geospark.spatialRDD.PointRDD


object DemoApp {
  def main(args: Array[String]): Unit = {
    // åˆ›å»ºSparkConf
    val conf = new SparkConf().
      setAppName("GeoSparkDemo1").
      setMaster("local[*]").
      set("spark.serializer", classOf[KryoSerializer].getName).
      set("spark.kryo.registrator", classOf[GeoSparkKryoRegistrator].getName)
    val sc = new SparkContext(conf)

    val pointRDDInputLocation = "data/checkin.csv"
    // è¿™ä¸ªå˜é‡æ§åˆ¶æˆ‘ä»¬çš„åœ°ç†ç»åº¦å’Œçº¬åº¦åœ¨æ•°æ®çš„å“ªä¸¤åˆ—ï¼Œæˆ‘ä»¬è¿™é‡Œæ˜¯ç¬¬0,1åˆ—ï¼ŒOffsetå°±è®¾ç½®ä¸º0
    val pointRDDOffset = 0
    val pointRDDSplitter = FileDataSplitter.CSV
    // è¿™ä¸ªå‚æ•°å…è®¸æˆ‘ä»¬é™¤äº†ç»çº¬åº¦å¤–è¿˜å¯ä»¥æºå¸¦å…¶ä»–è‡ªå®šä¹‰æ•°æ®
    val carryOtherAttributes = true
    val objectRDD = new PointRDD(sc, pointRDDInputLocation,pointRDDOffset, pointRDDSplitter, carryOtherAttributes)
    // è·å–rawRDDè¿›è¡Œéå†è¾“å‡º
    objectRDD.rawSpatialRDD.rdd.collect().foreach(println)
  }
}
```

Output:

![image](https://raw.githubusercontent.com/YutingYao/DailyJupyter/main/imageSever/image.2xpne7kb29g0.png)

é€šè¿‡GeometryFactoryåˆ›å»ºåœ°ç†æ•°æ®ï¼š

```js
package com.suddev.bigdata.core
import com.vividsolutions.jts.geom.{Coordinate, GeometryFactory}

object GeoDemoApp {
  def main(args: Array[String]): Unit = {
    // åˆ›å»ºä¸€ä¸ªåæ ‡
    val coord = new Coordinate(-84.01, 34.01)
    // å®ä¾‹åŒ–Geometryå·¥å‚ç±»
    val factory = new GeometryFactory()
    // åˆ›å»ºPoint
    val pointObject = factory.createPoint(coord)
    // åˆ›å»ºPolygon
    val coordinates = new Array[Coordinate](5)
    coordinates(0) = new Coordinate(0,0)
    coordinates(1) = new Coordinate(0,4)
    coordinates(2) = new Coordinate(4,4)
    coordinates(3) = new Coordinate(4,0)
    // å¤šè¾¹å½¢æ˜¯é—­åˆçš„ï¼Œæ‰€æœ‰æœ€åä¸€ä¸ªç‚¹å°±æ˜¯ç¬¬ä¸€ä¸ªç‚¹
    coordinates(4) = coordinates(0) 
    val polygonObject = factory.createPolygon(coordinates)
    // åˆ›å»ºLineString
    val coordinates2 = new Array[Coordinate](4)
    coordinates2(0) = new Coordinate(0,0)
    coordinates2(1) = new Coordinate(0,4)
    coordinates2(2) = new Coordinate(4,4)
    coordinates2(3) = new Coordinate(4,0)
    val linestringObject = factory.createLineString(coordinates2)
  }
}
```

### <a name='SpatialRDDSRDD'></a>2.8.3. åˆ›å»ºSpatialRDD(SRDD)

GeoSpark-Core æä¾›äº†ä¸‰ç§ç‰¹æ®Šçš„SpatialRDDï¼š

* PointRDD
* PolygonRDD
* LineStringRDD

å®ƒä»¬å¯ä»¥ä»Spark RDDï¼ŒCSVï¼ŒTSVï¼ŒWKTï¼ŒWKBï¼ŒShapefilesï¼ŒGeoJSONå’ŒNetCDF / HDFæ ¼å¼åŠ è½½ã€‚
è¿™é‡Œç»™å‡ºå‡ ç§å¸¸ç”¨åœºæ™¯ç¤ºä¾‹ï¼š

step 1. åˆå§‹åŒ–SparkContext

```sql
val conf = new SparkConf().
  setAppName("GeoSparkDemo2").
  setMaster("local[*]").
  set("spark.serializer", classOf[KryoSerializer].getName).
  set("spark.kryo.registrator", classOf[GeoSparkKryoRegistrator].getName)
val sc = new SparkContext(conf)
```

step 2. åˆ›å»ºtyped Spatial RDD - é€šè¿‡å·²æœ‰Spark RDDåˆ›å»ºPointRDD

```js
// æ•°æ®å‡†å¤‡
val data = Array(
      (-88.331492,32.324142,"hotel"),
      (-88.175933,32.360763,"gas"),
      (-88.388954,32.357073,"bar"),
      (-88.221102,32.35078,"restaurant")
    )
val geometryFactory = new GeometryFactory()
// åˆ›å»ºSpark RDD[Point]
val pointsRowSpatialRDD = sc.parallelize(data)
      .map(x => {
       // åˆ›å»ºåæ ‡
        val coord = new Coordinate(x._1, x._2)
        // ç”¨æˆ·å®šä¹‰æ•°æ®
        val userData = x._3
        // åˆ›å»ºPoint
        val point = geometryFactory.createPoint(coord)
        // Pointæ”¯æŒæºå¸¦ç”¨æˆ·æ•°æ®
        point.setUserData(userData)
        point
       })
// åˆ›å»ºPointRDD 
val pointRDD = new PointRDD(pointsRowSpatialRDD)
```

step 2. åˆ›å»ºtyped Spatial RDD - é€šè¿‡CSV/TSVåˆ›å»ºPointRDD

åˆ›å»ºcheckin.csvåœ¨ data/checkin.csvè·¯å¾„ä¸‹:

```js
-88.331492,32.324142,hotel
-88.175933,32.360763,gas
-88.388954,32.357073,bar
-88.221102,32.35078,restaurant
```

checkin.csvä¸€å…±æœ‰ä¸‰åˆ—(Column IDs) ä¸º 0, 1, 2.
ç¬¬0ï¼Œ1 åˆ—æ˜¯åæ ‡
ç¬¬2åˆ—æ˜¯ç”¨æˆ·å®šä¹‰æ•°æ®
pointRDDOffset æ§åˆ¶åœ°ç†åæ ‡ä»ç¬¬å‡ åˆ—å¼€å§‹ï¼Œæ•…offset=0

```js
val pointRDDInputLocation = "data/checkin.csv"
val pointRDDOffset = 0  // The coordinates start from Column 0
val pointRDDSplitter = FileDataSplitter.CSV // or use  FileDataSplitter.TSV
val carryOtherAttributes = true // æ”¯æŒæºå¸¦ç”¨æˆ·å®šä¹‰æ•°æ® (hotel, gas, bar...)
var objectRDD = new PointRDD(sc, pointRDDInputLocation, pointRDDOffset, pointRDDSplitter, carryOtherAttributes)
```

step 2. åˆ›å»ºtyped Spatial RDD - é€šè¿‡CSV/TSVåˆ›å»ºPolygonRDD/LineStringRDD

åˆ›å»ºcheckinshape.csvåœ¨ data/checkin.csvè·¯å¾„ä¸‹:

```js
-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,-88.331492,32.324142,hotel
-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,-88.175933,32.360763,gas
-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,-88.388954,32.357073,bar
-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,-88.221102,32.35078,restaurant
```

checkinshape.csvä¸€å…±æœ‰11åˆ—(Column IDs) ä¸º 0~10
ç¬¬0 - 9 åˆ—æ˜¯5ä¸ªåæ ‡
ç¬¬10åˆ—æ˜¯ç”¨æˆ·å®šä¹‰æ•°æ®
polygonRDDStartOffset æ§åˆ¶åœ°ç†åæ ‡ä»ç¬¬å‡ åˆ—å¼€å§‹ï¼Œæ•…StartOffset = 0
polygonRDDStartOffset æ§åˆ¶åœ°ç†åæ ‡ä»ç¬¬å‡ åˆ—ç»“æŸï¼Œæ•…EndOffset = 8

```js
val polygonRDDInputLocation = "data/checkinshape.csv"
val polygonRDDStartOffset = 0 // The coordinates start from Column 0
val polygonRDDEndOffset = 8 // The coordinates end at Column 8
val polygonRDDSplitter = FileDataSplitter.CSV // or use  FileDataSplitter.TSV
val carryOtherAttributes = true
var objectRDD = new PolygonRDD(sc, polygonRDDInputLocation, polygonRDDStartOffset, polygonRDDEndOffset, polygonRDDSplitter, carryOtherAttributes)
```

step 3. åˆ›å»ºé€šç”¨Spatial RDD

é€šç”¨SpatialRDDä¸åŒäºPointRDDï¼ŒPolygonRDDå’ŒLineStringRDDï¼Œ

å®ƒå…è®¸è¾“å…¥æ•°æ®æ–‡ä»¶åŒ…å«æ··åˆçš„å‡ ä½•ç±»å‹ï¼Œèƒ½å¤Ÿé€‚ç”¨æ›´å¤šåœºæ™¯ã€‚

WKT/WKB/GeoJson/Shapefileç­‰æ–‡ä»¶ç±»å‹å°±

å¯ä»¥æ”¯æŒä¿å­˜å¤šç§åœ°ç†æ•°æ®å¦‚ LineString, Polygonå’ŒMultiPolygon

step 3. åˆ›å»ºé€šç”¨Spatial RDD - é€šè¿‡WKT/WKBåˆ›å»º - checkin.tsv

```tsv
POINT(-88.331492 32.324142) hotel
POINT(-88.175933 32.360763) gas
POINT(-88.388954 32.357073) bar
POINT(-88.221102 32.35078) restaurant
```

ä»£ç ï¼š

```js
val inputLocation = "data/checkin.tsv"
val wktColumn = 0 // The WKT string starts from Column 0
val allowTopologyInvalidGeometries = true 
val skipSyntaxInvalidGeometries = false  
val spatialRDD = WktReader.readToGeometryRDD(sc, inputLocation, wktColumn, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)
```

step 3. åˆ›å»ºé€šç”¨Spatial RDD - é€šè¿‡GeoJSONåˆ›å»º - polygon.json

```json
{ "type": "Feature", "properties": { "STATEFP": "01", "COUNTYFP": "077", "TRACTCE": "011501", "BLKGRPCE": "5", "AFFGEOID": "1500000US010770115015", "GEOID": "010770115015", "NAME": "5", "LSAD": "BG", "ALAND": 6844991, "AWATER": 32636 }, "geometry": { "type": "Polygon", "coordinates": [ [ [ -87.621765, 34.873444 ], [ -87.617535, 34.873369 ], [ -87.6123, 34.873337 ], [ -87.604049, 34.873303 ], [ -87.604033, 34.872316 ], [ -87.60415, 34.867502 ], [ -87.604218, 34.865687 ], [ -87.604409, 34.858537 ], [ -87.604018, 34.851336 ], [ -87.603716, 34.844829 ], [ -87.603696, 34.844307 ], [ -87.603673, 34.841884 ], [ -87.60372, 34.841003 ], [ -87.603879, 34.838423 ], [ -87.603888, 34.837682 ], [ -87.603889, 34.83763 ], [ -87.613127, 34.833938 ], [ -87.616451, 34.832699 ], [ -87.621041, 34.831431 ], [ -87.621056, 34.831526 ], [ -87.62112, 34.831925 ], [ -87.621603, 34.8352 ], [ -87.62158, 34.836087 ], [ -87.621383, 34.84329 ], [ -87.621359, 34.844438 ], [ -87.62129, 34.846387 ], [ -87.62119, 34.85053 ], [ -87.62144, 34.865379 ], [ -87.621765, 34.873444 ] ] ] } },
{ "type": "Feature", "properties": { "STATEFP": "01", "COUNTYFP": "045", "TRACTCE": "021102", "BLKGRPCE": "4", "AFFGEOID": "1500000US010450211024", "GEOID": "010450211024", "NAME": "4", "LSAD": "BG", "ALAND": 11360854, "AWATER": 0 }, "geometry": { "type": "Polygon", "coordinates": [ [ [ -85.719017, 31.297901 ], [ -85.715626, 31.305203 ], [ -85.714271, 31.307096 ], [ -85.69999, 31.307552 ], [ -85.697419, 31.307951 ], [ -85.675603, 31.31218 ], [ -85.672733, 31.312876 ], [ -85.672275, 31.311977 ], [ -85.67145, 31.310988 ], [ -85.670622, 31.309524 ], [ -85.670729, 31.307622 ], [ -85.669876, 31.30666 ], [ -85.669796, 31.306224 ], [ -85.670356, 31.306178 ], [ -85.671664, 31.305583 ], [ -85.67177, 31.305299 ], [ -85.671878, 31.302764 ], [ -85.671344, 31.302123 ], [ -85.668276, 31.302076 ], [ -85.66566, 31.30093 ], [ -85.665687, 31.30022 ], [ -85.669183, 31.297677 ], [ -85.668703, 31.295638 ], [ -85.671985, 31.29314 ], [ -85.677177, 31.288211 ], [ -85.678452, 31.286376 ], [ -85.679236, 31.28285 ], [ -85.679195, 31.281426 ], [ -85.676865, 31.281049 ], [ -85.674661, 31.28008 ], [ -85.674377, 31.27935 ], [ -85.675714, 31.276882 ], [ -85.677938, 31.275168 ], [ -85.680348, 31.276814 ], [ -85.684032, 31.278848 ], [ -85.684387, 31.279082 ], [ -85.692398, 31.283499 ], [ -85.705032, 31.289718 ], [ -85.706755, 31.290476 ], [ -85.718102, 31.295204 ], [ -85.719132, 31.29689 ], [ -85.719017, 31.297901 ] ] ] } },
{ "type": "Feature", "properties": { "STATEFP": "01", "COUNTYFP": "055", "TRACTCE": "001300", "BLKGRPCE": "3", "AFFGEOID": "1500000US010550013003", "GEOID": "010550013003", "NAME": "3", "LSAD": "BG", "ALAND": 1378742, "AWATER": 247387 }, "geometry": { "type": "Polygon", "coordinates": [ [ [ -86.000685, 34.00537 ], [ -85.998837, 34.009768 ], [ -85.998012, 34.010398 ], [ -85.987865, 34.005426 ], [ -85.986656, 34.004552 ], [ -85.985, 34.002659 ], [ -85.98851, 34.001502 ], [ -85.987567, 33.999488 ], [ -85.988666, 33.99913 ], [ -85.992568, 33.999131 ], [ -85.993144, 33.999714 ], [ -85.994876, 33.995153 ], [ -85.998823, 33.989548 ], [ -85.999925, 33.994237 ], [ -86.000616, 34.000028 ], [ -86.000685, 34.00537 ] ] ] } },
{ "type": "Feature", "properties": { "STATEFP": "01", "COUNTYFP": "089", "TRACTCE": "001700", "BLKGRPCE": "2", "AFFGEOID": "1500000US010890017002", "GEOID": "010890017002", "NAME": "2", "LSAD": "BG", "ALAND": 1040641, "AWATER": 0 }, "geometry": { "type": "Polygon", "coordinates": [ [ [ -86.574172, 34.727375 ], [ -86.562684, 34.727131 ], [ -86.562797, 34.723865 ], [ -86.562957, 34.723168 ], [ -86.562336, 34.719766 ], [ -86.557381, 34.719143 ], [ -86.557352, 34.718322 ], [ -86.559921, 34.717363 ], [ -86.564827, 34.718513 ], [ -86.567582, 34.718565 ], [ -86.570572, 34.718577 ], [ -86.573618, 34.719377 ], [ -86.574172, 34.727375 ] ] ] } },
```

ä»£ç ï¼š

```js
val inputLocation = "data/polygon.json"
val allowTopologyInvalidGeometries = true 
val skipSyntaxInvalidGeometries = false
val spatialRDD = GeoJsonReader.readToGeometryRDD(sc, inputLocation, allowTopologyInvalidGeometries, skipSyntaxInvalidGeometries)
```

step 3. åˆ›å»ºé€šç”¨Spatial RDD - é€šè¿‡Shapefileåˆ›å»º

```js
val shapefileInputLocation="data/myshapefile"
// System.setProperty("geospark.global.charset", "utf8")
val spatialRDD = ShapefileReader.readToGeometryRDD(sc, shapefileInputLocation)
```

æ³¨æ„:

.shp, .shx, .dbf æ–‡ä»¶åç¼€å¿…é¡»æ˜¯å°å†™. å¹¶ä¸” shapefile æ–‡ä»¶å¿…é¡»å‘½åä¸ºmyShapefile, æ–‡ä»¶å¤¹ç»“æ„å¦‚ä¸‹:

```js
- shapefile1
- shapefile2
- myshapefile
    - myshapefile.shp
    - myshapefile.shx
    - myshapefile.dbf
    - myshapefile...
    - ...
```

å¦‚æœå‡ºç°ä¹±ç é—®é¢˜å¯ä»¥åœ¨ShapefileReader.readToGeometryRDDæ–¹æ³•è°ƒç”¨ä¹‹å‰è®¾ç½®ç¼–ç å‚æ•°

```js
System.setProperty("geospark.global.charset", "utf8")
```

step 4. åæ ‡ç³»è½¬æ¢

GeoSparké‡‡ç”¨EPGSæ ‡å‡†åæ ‡ç³»ï¼Œå…¶åæ ‡ç³»ä¹Ÿå¯å‚è€ƒEPSGå®˜ç½‘ï¼š<https://epsg.io/>

å¦‚æœéœ€è¦è½¬æ¢æˆå…¶ä»–æ ‡å‡†çš„åæ ‡ç³»ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•

```js
// æºæ ‡å‡†
val sourceCrsCode = "epsg:4326"
// ç›®æ ‡æ ‡å‡†
val targetCrsCode = "epsg:3857"
objectRDD.CRSTransform(sourceCrsCode, targetCrsCode)
```

### <a name='SpatialRangeQuery'></a>2.8.4. ç©ºé—´èŒƒå›´æŸ¥è¯¢(Spatial Range Query)

ç©ºé—´èŒƒå›´æŸ¥è¯¢ï¼Œé¡¾åæ€ä¹‰æˆ‘ä»¬å¯ä»¥ç»™å®šä¸€ä¸ªèŒƒå›´ï¼ˆquery windowï¼‰ï¼Œç„¶åæŸ¥è¯¢å‡ºåŒ…å«åœ¨å½“å‰èŒƒå›´å†…çš„åœ°ç†å¯¹è±¡ã€‚

1.1 æ•°æ®å‡†å¤‡

åˆ›å»ºcheckin1.csvåœ¨ data/checkin1.csvè·¯å¾„ä¸‹:
æ³¨æ„è¿™é‡Œæ•…æ„æŠŠbaråæ ‡ä¿®æ”¹äº†ä¸€ä¸‹

```js
-88.331492,32.324142,hotel
-88.175933,32.360763,gas
-99.388954,32.357073,bar
-88.221102,32.35078,restaurant
```

1.2 ä»£ç ç¤ºä¾‹

considerBoundaryIntersectionå‚æ•°å¯ä»¥é…ç½®æŸ¥è¯¢æ˜¯å¦åŒ…æ‹¬query windowè¾¹ç•Œä¸Šçš„åœ°ç†å¯¹è±¡ã€‚

```js
package com.suddev.bigdata.query

import com.vividsolutions.jts.geom.Envelope
import org.apache.spark.serializer.KryoSerializer
import org.apache.spark.{SparkConf, SparkContext}
import org.datasyslab.geospark.enums.FileDataSplitter
import org.datasyslab.geospark.serde.GeoSparkKryoRegistrator
import org.datasyslab.geospark.spatialOperator.RangeQuery
import org.datasyslab.geospark.spatialRDD.PointRDD

/**
 * Spatial Range Query
 * @author Rand
 * @date 2020/4/16 0016
 */
object SpatialRangeQueryApp {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().
      setAppName("SpatialRangeQueryApp").setMaster("local[*]").
      set("spark.serializer",classOf[KryoSerializer].getName).
      set("spark.kryo.registrator", classOf[GeoSparkKryoRegistrator].getName)
    implicit val sc = new SparkContext(conf)
    val objectRDD = createPointRDD
    objectRDD.rawSpatialRDD.rdd.collect().foreach(println)

    // å®šä¹‰QueryWindow
    val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01)
    // æ˜¯å¦è€ƒè™‘è¾¹ç•Œ
    val considerBoundaryIntersection = false
    val usingIndex = false
    val queryResult = RangeQuery.SpatialRangeQuery(objectRDD, rangeQueryWindow, considerBoundaryIntersection, usingIndex)
    queryResult.rdd.collect().foreach(println)
  }

  def createPointRDD(implicit sc:SparkContext): PointRDD ={
    val pointRDDInputLocation = "data/checkin1.csv"
    // è¿™ä¸ªå˜é‡æ§åˆ¶æˆ‘ä»¬çš„åœ°ç†ç»åº¦å’Œçº¬åº¦åœ¨æ•°æ®çš„å“ªä¸¤åˆ—ï¼Œæˆ‘ä»¬è¿™é‡Œæ˜¯ç¬¬0,1åˆ—ï¼ŒOffsetå°±è®¾ç½®ä¸º0
    val pointRDDOffset = 0
    val pointRDDSplitter = FileDataSplitter.CSV
    // è¿™ä¸ªå‚æ•°å…è®¸æˆ‘ä»¬é™¤äº†ç»çº¬åº¦å¤–è¿˜å¯ä»¥æºå¸¦å…¶ä»–è‡ªå®šä¹‰æ•°æ®
    val carryOtherAttributes = true
    val objectRDD = new PointRDD(sc, pointRDDInputLocation,pointRDDOffset, pointRDDSplitter, carryOtherAttributes)
    objectRDD
  }
}
```

è¿™é‡Œçš„rangeQueryWindowé™¤äº†æ”¯æŒEnvelopeå¤–è¿˜å¯ä»¥ä½¿ç”¨Point/Polygon/LineString

ç‚¹->åˆ›å»ºä¸€ä¸ªPoint Query Windowï¼š

```js
val geometryFactory = new GeometryFactory()
val pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))
```

å¤šè¾¹å½¢->åˆ›å»ºä¸€ä¸ªPolygon Query Windowï¼š

```js
val geometryFactory = new GeometryFactory()
val coordinates = new Array[Coordinate](5)
coordinates(0) = new Coordinate(0,0)
coordinates(1) = new Coordinate(0,4)
coordinates(2) = new Coordinate(4,4)
coordinates(3) = new Coordinate(4,0)
coordinates(4) = coordinates(0) // The last coordinate is the same as the first coordinate in order to compose a closed ring
val polygonObject = geometryFactory.createPolygon(coordinates)
```

çº¿->åˆ›å»ºä¸€ä¸ªLinestring Query Windowï¼š

```js
val geometryFactory = new GeometryFactory()
val coordinates = new Array[Coordinate](5)
coordinates(0) = new Coordinate(0,0)
coordinates(1) = new Coordinate(0,4)
coordinates(2) = new Coordinate(4,4)
coordinates(3) = new Coordinate(4,0)
val linestringObject = geometryFactory.createLineString(coordinates)
```

1.3 è¿è¡Œæ•ˆæœ

å¯ä»¥çœ‹åˆ°æŸ¥è¯¢ç»“æœåŒ…å«hotel,gas,restaurantä¸åŒ…å«bar

```js
POINT (-88.331492 32.324142) hotel
POINT (-88.175933 32.360763) gas
POINT (-99.388954 32.357073) bar
POINT (-88.221102 32.35078) restaurant
-------------------------------
POINT (-88.331492 32.324142) hotel
POINT (-88.175933 32.360763) gas
POINT (-88.221102 32.35078) restaurant
-------------------------------
```

2.ç©ºé—´ä¸´è¿‘æŸ¥è¯¢(Spatial KNN Query)

ç©ºé—´ä¸´è¿‘ç®—æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ç»™çš„ä¸€ä¸ªä¸­å¿ƒç‚¹çš„åæ ‡ï¼Œç„¶åæ‰¾å‡ºè¯¥ç‚¹ç›¸é‚»çš„Kä¸ªåœ°ç†å¯¹è±¡

2.1 æ•°æ®å‡†å¤‡

åˆ›å»ºcheckin2.csvåœ¨ data/checkin2.csvè·¯å¾„ä¸‹:

```js
-88.331492,32.324142,hotel
-88.175933,32.360763,gas1
-88.176033,32.360763,gas2
-88.175833,32.360763,gas3
-88.388954,32.357073,bar
-88.221102,32.35078,restaurant
```

2.2 ä»£ç ç¤ºä¾‹

kå‚æ•°å¯ä»¥è®¾ç½®é™åˆ¶æŸ¥è¯¢kä¸ªç»“æœ

ğŸ™ƒè¿™é‡Œåæ§½ä¸€ä¸‹ï¼Œå¦‚æœæŸ¥è¯¢ç»“æœä¸º5ä¸ªï¼Œä½†æ˜¯æˆ‘ä»¬kè®¾ç½®çš„å¤§äº5å°±ä¼šæŠ¥ç©ºæŒ‡é’ˆå¼‚å¸¸hhhï¼Œä¸èƒ½æŸ¥åˆ°å¤šå°‘è¿”å›å¤šå°‘ä¹ˆ

ğŸ™ƒå†åæ§½ä¸€ä¸‹ï¼Œå®ƒè¿™ç§è®¾è®¡ä¸€æ¬¡åªèƒ½æŸ¥è¯¢ä¸€ä¸ªç‚¹ï¼Œå®é™…ç”Ÿäº§ä¸Šè‚¯å®šæ˜¯ä¸€æ‰¹ç‚¹å’Œå¦å¤–ä¸€æ‰¹ç‚¹åšKNNåŒ¹é…ï¼Œè€Œä»–è¿™ä¸ªä¸æ”¯æŒä¸¤ä¸ªRDDæŸ¥è¯¢ï¼Œå¦‚æœæœ‰æ„Ÿå…´è¶£çš„ä¸¤ä¸ªRDDåšKNNåŒ¹é…çš„è¯·ç»™æˆ‘ç•™è¨€ï¼Œæˆ‘å•ç‹¬å†™ä¸€ç¯‡æ–‡ç« 

```js
package com.suddev.bigdata.query

import com.vividsolutions.jts.geom.{Coordinate, Envelope, GeometryFactory}
import org.apache.spark.serializer.KryoSerializer
import org.apache.spark.{SparkConf, SparkContext}
import org.datasyslab.geospark.enums.FileDataSplitter
import org.datasyslab.geospark.serde.GeoSparkKryoRegistrator
import org.datasyslab.geospark.spatialOperator.{KNNQuery, RangeQuery}
import org.datasyslab.geospark.spatialRDD.PointRDD
import scala.collection.JavaConversions._

/**
 * SpatialKNNQueryApp
 * @author Rand
 * @date 2020/4/16 0016
 */
object SpatialKNNQueryApp {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().
      setAppName("SpatialKNNQueryApp").setMaster("local[*]").
      set("spark.serializer",classOf[KryoSerializer].getName).
      set("spark.kryo.registrator", classOf[GeoSparkKryoRegistrator].getName)
    implicit val sc = new SparkContext(conf)
    val objectRDD = createPointRDD
    objectRDD.rawSpatialRDD.rdd.collect().foreach(println)
    val geometryFactory = new GeometryFactory()
    // åšä¸´è¿‘æŸ¥è¯¢çš„ä¸­å¿ƒç‚¹
    val pointObject = geometryFactory.createPoint(new Coordinate(-84.01, 34.01))
    val K = 2 // K Nearest Neighbors
    val usingIndex = false
    val result = KNNQuery.SpatialKnnQuery(objectRDD, pointObject, K, usingIndex)
    println("-----------------------------------")
    // è®°å¾—import scala.collection.JavaConversions._ å¦åˆ™è¿™é‡ŒæŠ¥é”™å“ˆ
    result.foreach(println)
  }

  def createPointRDD(implicit sc:SparkContext): PointRDD ={
    val pointRDDInputLocation = "data/checkin2.csv"
    // è¿™ä¸ªå˜é‡æ§åˆ¶æˆ‘ä»¬çš„åœ°ç†ç»åº¦å’Œçº¬åº¦åœ¨æ•°æ®çš„å“ªä¸¤åˆ—ï¼Œæˆ‘ä»¬è¿™é‡Œæ˜¯ç¬¬0,1åˆ—ï¼ŒOffsetå°±è®¾ç½®ä¸º0
    val pointRDDOffset = 0
    val pointRDDSplitter = FileDataSplitter.CSV
    // è¿™ä¸ªå‚æ•°å…è®¸æˆ‘ä»¬é™¤äº†ç»çº¬åº¦å¤–è¿˜å¯ä»¥æºå¸¦å…¶ä»–è‡ªå®šä¹‰æ•°æ®
    val carryOtherAttributes = true
    val objectRDD = new PointRDD(sc, pointRDDInputLocation,pointRDDOffset, pointRDDSplitter, carryOtherAttributes)
    objectRDD
  }
}
```

2.3 è¿è¡Œæ•ˆæœ

å¯ä»¥çœ‹åˆ°æŸ¥è¯¢ç»“æœåŒ…å«gas3ï¼Œgas1ä¸¤ä¸ªç‚¹

```js
POINT (-88.331492 32.324142) hotel
POINT (-88.175933 32.360763) gas1
POINT (-88.176033 32.360763) gas2
POINT (-88.175833 32.360763) gas3
POINT (-88.388954 32.357073) bar
POINT (-88.221102 32.35078) restaurant
-----------------------------------
POINT (-88.175833 32.360763) gas3
POINT (-88.175933 32.360763) gas1
```

3.ç©ºé—´è¿æ¥æŸ¥è¯¢(Spatial Join Query)

ç©ºé—´è¿æ¥æŸ¥è¯¢ç®—æ³•ï¼Œç±»ä¼¼äºæ•°æ®åº“ä¸­çš„Joinæ“ä½œï¼Œ

æœ‰Spatial RDD A and Bï¼Œéå†Aä¸­çš„å‡ ä½•å¯¹è±¡å»åŒ¹é…Bä¸­è¦†ç›–æˆ–ç›¸äº¤çš„å‡ ä½•å¯¹è±¡ã€‚

3.1 æ•°æ®å‡†å¤‡

åˆ›å»ºcheckin3.csvåœ¨ data/checkin3.csvè·¯å¾„ä¸‹:

```js
-88.331492,32.324142,1.hotel
-88.175933,32.360763,1.gas
-88.388954,32.357073,1.bar
-88.588954,32.357073,1.spark
```

åˆ›å»ºcheckin4.csvåœ¨ data/checkin4.csvè·¯å¾„ä¸‹:

```js
-88.175933,32.360763,2.gas
-88.388954,32.357073,2.bar
-88.221102,32.35078,2.restaurant
-88.321102,32.35078,2.bus
```

3.2 ä»£ç ç¤ºä¾‹

```js
package com.suddev.bigdata.query

import org.apache.spark.serializer.KryoSerializer
import org.apache.spark.{SparkConf, SparkContext}
import org.datasyslab.geospark.enums.{FileDataSplitter, GridType}
import org.datasyslab.geospark.serde.GeoSparkKryoRegistrator
import org.datasyslab.geospark.spatialOperator.JoinQuery
import org.datasyslab.geospark.spatialRDD.PointRDD
/**
 * SpatialJoinQueryApp
 *
 * @author Rand
 * @date 2020/4/16 0016
 */
object SpatialJoinQueryApp {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().
      setAppName("SpatialJoinQueryApp").setMaster("local[*]").
      set("spark.serializer",classOf[KryoSerializer].getName).
      set("spark.kryo.registrator", classOf[GeoSparkKryoRegistrator].getName)
    implicit val sc = new SparkContext(conf)
    // å‡†å¤‡æ•°æ®
    val objectRDD = createObjectRDDRDD
    objectRDD.rawSpatialRDD.rdd.collect().foreach(println)
    val queryWindowRDD = createQueryWindowRDD
    println("---------------------------")
    queryWindowRDD.rawSpatialRDD.rdd.collect().foreach(println)
    println("---------------------------")
    objectRDD.analyze()
    // å¿…é¡»è®¾ç½®objectRDDå’ŒqueryWindowRDDçš„spatialPartitioning
    // æ¡ä»¶æœ‰äºŒ
    // 1.objectRDDå’ŒqueryWindowRDDçš„spatialPartitioning å¿…é¡»éç©ºç›¸åŒ
    // 2.objectRDDå’ŒqueryWindowRDDçš„åˆ†åŒºæ•°é‡å¿…é¡»ä¸€æ ·
    objectRDD.spatialPartitioning(GridType.KDBTREE)
    queryWindowRDD.spatialPartitioning(objectRDD.getPartitioner)
    val considerBoundaryIntersection = false
    val usingIndex = false
    val result = JoinQuery.SpatialJoinQuery(objectRDD, queryWindowRDD, usingIndex, considerBoundaryIntersection)
    result.rdd.foreach(println)
  }

  def createObjectRDDRDD(implicit sc:SparkContext): PointRDD ={
    val pointRDDInputLocation = "data/checkin3.csv"
    val pointRDDOffset = 0
    val pointRDDSplitter = FileDataSplitter.CSV
    val carryOtherAttributes = true
    val objectRDD = new PointRDD(sc, pointRDDInputLocation,pointRDDOffset, pointRDDSplitter, carryOtherAttributes)
    objectRDD
  }

  def createQueryWindowRDD(implicit sc:SparkContext): PointRDD ={
    val pointRDDInputLocation = "data/checkin4.csv"
    val pointRDDOffset = 0
    val pointRDDSplitter = FileDataSplitter.CSV
    val carryOtherAttributes = true
    val objectRDD = new PointRDD(sc, pointRDDInputLocation,pointRDDOffset, pointRDDSplitter, carryOtherAttributes)
    objectRDD
  }
}
```

3.3 è¿è¡Œæ•ˆæœ

å¯ä»¥çœ‹åˆ°ä¸¤è¾¹çš„gasï¼ŒbarJoinå…³è”ä¸Šäº†

```js
POINT (-88.331492 32.324142) 1.hotel
POINT (-88.175933 32.360763) 1.gas
POINT (-88.388954 32.357073) 1.bar
POINT (-88.588954 32.357073) 1.spark
---------------------------
POINT (-88.175933 32.360763) 2.gas
POINT (-88.388954 32.357073) 2.bar
POINT (-88.221102 32.35078) 2.restaurant
POINT (-88.321102 32.35078) 2.bus
---------------------------
(POINT (-88.175933 32.360763) 2.gas,[POINT (-88.175933 32.360763) 1.gas])
(POINT (-88.388954 32.357073) 2.bar,[POINT (-88.388954 32.357073) 1.bar])
```

4.è·ç¦»è¿æ¥æŸ¥è¯¢(Distance Join Query)

è·ç¦»è”æ¥æŸ¥è¯¢å°†ä¸¤ä¸ªSpatial RDD Aå’ŒBå’Œä¸€ä¸ªè·ç¦»ä½œä¸ºè¾“å…¥ã€‚

å¯¹äºAä¸­çš„æ¯ä¸ªå‡ ä½•å¯¹è±¡ï¼Œæ‰¾åˆ°Bä¸­éƒ½åœ¨ç»™å®šè·ç¦»ä¹‹å†…çš„é›†åˆå¯¹è±¡ã€‚

âš ï¸å…³äºè·ç¦»è¯´æ˜ï¼š

GeoSparkä¸ä¼šæ§åˆ¶SpatialRDDä¸­æ‰€æœ‰å‡ ä½•çš„åæ ‡å•ä½ï¼ˆåŸºäºåº¦æˆ–åŸºäºç±³ï¼‰ã€‚

GeoSparkä¸­æ‰€æœ‰ç›¸å…³è·ç¦»çš„å•ä½ä¸SpatialRDDä¸­æ‰€æœ‰å‡ ä½•çš„å•ä½ï¼ˆï¼‰ç›¸åŒã€‚

è½¬æ¢å‚è€ƒåæ ‡ç³»ï¼ˆCoordinate Reference Systemï¼‰ä»£ç :

```js
val sourceCrsCode = "epsg:4326" // WGS84, the most common degree-based CRS
val targetCrsCode = "epsg:3857" // The most common meter-based CRS
objectRDD.CRSTransform(sourceCrsCode, targetCrsCode)
```

4.1 æ•°æ®å‡†å¤‡

åˆ›å»ºcheckin5.csvåœ¨ data/checkin5.csvè·¯å¾„ä¸‹:

```js
-89.331492,32.324142,1.hotel
-88.1760,32.360763,1.gas
-88.3890,32.357073,1.bar
-89.588954,32.357073,1.spark
```

åˆ›å»ºcheckin6.csvåœ¨ data/checkin6.csvè·¯å¾„ä¸‹:

```js
-88.175933,32.360763,2.gas
-88.388954,32.357073,2.bar
-88.221102,32.35078,2.restaurant
-88.321102,32.35078,2.bus
```

4.2 ä»£ç ç¤ºä¾‹

```js
package com.suddev.bigdata.query

import org.apache.spark.serializer.KryoSerializer
import org.apache.spark.{SparkConf, SparkContext}
import org.datasyslab.geospark.enums.{FileDataSplitter, GridType}
import org.datasyslab.geospark.serde.GeoSparkKryoRegistrator
import org.datasyslab.geospark.spatialOperator.JoinQuery
import org.datasyslab.geospark.spatialRDD.{CircleRDD, PointRDD}

/**
 * DistanceJoinQueryApp
 *
 * @author Rand
 * @date 2020/4/16 0016
 */
object DistanceJoinQueryApp {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().
      setAppName("DistanceJoinQueryApp$").setMaster("local[*]").
      set("spark.serializer",classOf[KryoSerializer].getName).
      set("spark.kryo.registrator", classOf[GeoSparkKryoRegistrator].getName)
    implicit val sc = new SparkContext(conf)
    // å‡†å¤‡æ•°æ®
    val objectRddA = createObjectRDDA
    objectRddA.rawSpatialRDD.rdd.collect().foreach(println)
    val objectRddB = createObjectRDDB
    println("---------------------------")
    objectRddB.rawSpatialRDD.rdd.collect().foreach(println)
    println("---------------------------")
    // è®¾ç½®è·ç¦»
    val circleRDD = new CircleRDD(objectRddA, 0.1) // Create a CircleRDD using the given distance
    circleRDD.analyze()
    circleRDD.spatialPartitioning(GridType.KDBTREE)
    objectRddB.spatialPartitioning(circleRDD.getPartitioner)

    val considerBoundaryIntersection = false // Only return gemeotries fully covered by each query window in queryWindowRDD
    val usingIndex = false

    val result = JoinQuery.DistanceJoinQueryFlat(objectRddB, circleRDD, usingIndex, considerBoundaryIntersection)
    result.rdd.foreach(println)
  }

  def createObjectRDDA(implicit sc:SparkContext): PointRDD ={
    val pointRDDInputLocation = "data/checkin5.csv"
    val pointRDDOffset = 0
    val pointRDDSplitter = FileDataSplitter.CSV
    val carryOtherAttributes = true
    val objectRDD = new PointRDD(sc, pointRDDInputLocation,pointRDDOffset, pointRDDSplitter, carryOtherAttributes)
    objectRDD
  }

  def createObjectRDDB(implicit sc:SparkContext): PointRDD ={
    val pointRDDInputLocation = "data/checkin6.csv"
    val pointRDDOffset = 0
    val pointRDDSplitter = FileDataSplitter.CSV
    val carryOtherAttributes = true
    val objectRDD = new PointRDD(sc, pointRDDInputLocation,pointRDDOffset, pointRDDSplitter, carryOtherAttributes)
    objectRDD
  }
}
```

4.3 è¿è¡Œæ•ˆæœ

å¯ä»¥çœ‹åˆ°

1.gasåŒ¹é…åˆ°äº†2.gas,2.restaurantä¸¤ä¸ªç‚¹

1.baråŒ¹é…åˆ°äº†2.bar,2.busä¸¤ä¸ªç‚¹

```js
POINT (-89.331492 32.324142) 1.hotel
POINT (-88.176 32.360763) 1.gas
POINT (-88.389 32.357073) 1.bar
POINT (-89.588954 32.357073) 1.spark
---------------------------
POINT (-88.175933 32.360763) 2.gas
POINT (-88.388954 32.357073) 2.bar
POINT (-88.221102 32.35078) 2.restaurant
POINT (-88.321102 32.35078) 2.bus
---------------------------
(POINT (-88.176 32.360763) 1.gas,POINT (-88.175933 32.360763) 2.gas)
(POINT (-88.176 32.360763) 1.gas,POINT (-88.221102 32.35078) 2.restaurant)
(POINT (-88.389 32.357073) 1.bar,POINT (-88.388954 32.357073) 2.bar)
(POINT (-88.389 32.357073) 1.bar,POINT (-88.321102 32.35078) 2.bus)
```

```js

```

```js

```

```js

```




```

```sh

```

```sh

```

## <a name='flink'></a>2.10. flink

```sh
```

## <a name='PostgreSQL'></a>2.11. PostgreSQL

  Ubuntué»˜è®¤åŒ…å«PostgreSQLã€‚è¦åœ¨Ubuntuä¸Šå®‰è£…PostgreSQLï¼Œè¯·ä½¿ç”¨apt getï¼ˆæˆ–å…¶ä»–apté©±åŠ¨ï¼‰å‘½ä»¤ï¼š

  ```s
  apt-get install postgresql-12
  ```

PostgreSQL vs MongoDB

[postgresqlçš„é€Ÿåº¦æ¯”MongoDBæ›´å¿«](http://blog.chinaunix.net/uid-69999418-id-5848402.html)

[Mongodbä¸PostgreSQL+postgisç›¸æ¯”ï¼Œå„è‡ªçš„ä¼˜åŠ£åŠ¿æ˜¯ä»€ä¹ˆ?](https://www.zhihu.com/question/47292026)

Sparkå¯¹æ¥

[Sparkå¯¹æ¥åˆ†æå‹æ•°æ®åº“PostgreSQLç‰ˆå¿«é€Ÿå…¥é—¨](https://help.aliyun.com/document_detail/118439.html)

[Spark jdbc postgresqlæ•°æ®åº“è¿æ¥å’Œå†™å…¥æ“ä½œæºä»£ç è§£è¯»](https://my.oschina.net/u/4363935/blog/4026459)

[Spark jdbc postgresqlæ•°æ®åº“è¿æ¥å’Œå†™å…¥æ“ä½œæºä»£ç è§£è¯»](https://www.cnblogs.com/zhchoutai/p/8677027.html)

[Sparkè¯»å†™postgresql](https://blog.csdn.net/weixin_40450867/article/details/102613275)

flinkè¿æ¥

[flinkè¿æ¥postgresqlæ•°æ®åº“](https://blog.csdn.net/weixin_43315211/article/details/88354331)

[Flink-cdcå®æ—¶è¯»postgresql](https://www.cnblogs.com/xiongmozhou/p/14817641.html)

[flink cdcæ•è·postgresqlæ•°æ®](https://blog.csdn.net/weixin_41197407/article/details/112655218)

[åˆ›å»ºåˆ†æå‹æ•°æ®åº“PostgreSQLç‰ˆç»“æœè¡¨](https://help.aliyun.com/knowledge_detail/162453.html)

## <a name='MongoDB'></a>2.12. MongoDB

[MongoDB å¦‚ä½•ä¸Šæ‰‹å’Œé¿å‘ï¼Ÿ](https://mp.weixin.qq.com/s/EhVsdlRQDC1VP1S1QQfnkg)

[MongoDB ä¸å¾—ä¸çŸ¥çš„ 12 ä¸ªçŸ¥è¯†ç‚¹](https://mp.weixin.qq.com/s/EMHKgo2R8z8uyjAksK8hIQ)

[å½“ç‰©æµè¡Œä¸šé‡è§MongoDB](https://mp.weixin.qq.com/s/SHn_YLqR0Wzu8OF_j21PIA)

[äº‘MongoDB ä¼˜åŒ–è®©LBSæœåŠ¡æ€§èƒ½æå‡åå€](https://mp.weixin.qq.com/s/mCIL100G1GGNcxNHJiSKUQ)

**HDFS vs. MongoDB**

éƒ½æ˜¯åŸºäºå»‰ä»·**x86æœåŠ¡å™¨**çš„æ¨ªå‘æ‰©å±•æ¶æ„ï¼Œ

éƒ½èƒ½æ”¯æŒåˆ°**TBåˆ°PBçº§**çš„æ•°æ®é‡ã€‚æ•°æ®ä¼šåœ¨**å¤šèŠ‚ç‚¹è‡ªåŠ¨å¤‡ä»½**ï¼Œæ¥ä¿è¯æ•°æ®çš„**é«˜å¯ç”¨å’Œå†—ä½™**ã€‚ä¸¤è€…éƒ½æ”¯æŒ**éç»“æ„åŒ–æ•°æ®çš„å­˜å‚¨**ï¼Œç­‰ç­‰ã€‚

**ä½†æ˜¯ï¼ŒHDFSå’ŒMongoDBæ›´å¤šçš„æ˜¯å·®å¼‚ç‚¹ï¼š**

* å¦‚åœ¨å­˜å‚¨æ–¹å¼ä¸Š HDFSçš„å­˜å‚¨æ˜¯ä»¥æ–‡ä»¶ä¸ºå•ä½ï¼Œæ¯ä¸ªæ–‡ä»¶64MBåˆ°128MBä¸ç­‰ã€‚è€ŒMongoDBåˆ™æ˜¯**ç»†é¢—ç²’åŒ–çš„ã€ä»¥æ–‡æ¡£ä¸ºå•ä½**çš„å­˜å‚¨ã€‚

* HDFSä¸æ”¯æŒç´¢å¼•çš„æ¦‚å¿µï¼Œå¯¹æ•°æ®çš„æ“ä½œå±€é™äºæ‰«ææ€§è´¨çš„è¯»ï¼ŒMongoDBåˆ™æ”¯æŒåŸºäº**äºŒçº§ç´¢å¼•çš„å¿«é€Ÿæ£€ç´¢**ã€‚

* MongoDBå¯ä»¥æ”¯æŒå¸¸è§çš„**å¢åˆ æ”¹æŸ¥åœºæ™¯**ï¼Œè€ŒHDFSä¸€èˆ¬åªæ˜¯ä¸€æ¬¡å†™å…¥åå°±å¾ˆéš¾è¿›è¡Œä¿®æ”¹ã€‚

* ä»å“åº”æ—¶é—´ä¸Šæ¥è¯´ï¼ŒHDFSä¸€èˆ¬æ˜¯åˆ†é’Ÿçº§åˆ«è€ŒMongoDBå¯¹æ‰‹è¯·æ±‚çš„å“åº”æ—¶é—´é€šå¸¸ä»¥**æ¯«ç§’ä½œä¸ºå•ä½**ã€‚

å¦‚æœæœ‰ä¸€å¤©ä½ çš„ç»ç†å‘Šè¯‰ä½ ï¼š

ä»–æƒ³çŸ¥é“ç½‘ç«™ä¸Šæ¯å¤©æœ‰å¤šå°‘404é”™è¯¯åœ¨å‘ç”Ÿï¼Œ

è¿™ä¸ªæ—¶å€™å¦‚æœä½ ç”¨HDFSï¼Œå°±è¿˜æ˜¯éœ€è¦é€šè¿‡å…¨é‡æ‰«ææ‰€æœ‰è¡Œï¼Œ

è€ŒMongoDBåˆ™å¯ä»¥é€šè¿‡ç´¢å¼•ï¼Œå¾ˆå¿«åœ°æ‰¾åˆ°æ‰€æœ‰çš„404æ—¥å¿—ï¼Œå¯èƒ½èŠ±æ•°ç§’é’Ÿå°±å¯ä»¥è§£ç­”ä½ ç»ç†çš„é—®é¢˜ã€‚

åˆæ¯”å¦‚è¯´ï¼Œå¦‚æœä½ å¸Œæœ›å¯¹æ¯ä¸ªæ—¥å¿—é¡¹åŠ ä¸€ä¸ªè‡ªå®šä¹‰çš„å±æ€§ï¼Œ

åœ¨è¿›è¡Œä¸€äº›é¢„å¤„ç†åï¼ŒMongoDBå°±ä¼šæ¯”è¾ƒå®¹åœ°æ”¯æŒåˆ°ã€‚è€Œä¸€èˆ¬æ¥è¯´ï¼ŒHDFSæ˜¯ä¸æ”¯æŒæ›´æ–°ç±»å‹æ“ä½œçš„ã€‚

### <a name='MongoSparkConnector'></a>2.12.1. Mongo Spark Connector è¿æ¥å™¨

åœ¨è¿™é‡Œæˆ‘ä»¬åœ¨ä»‹ç»ä¸‹MongoDBå®˜æ–¹æä¾›çš„Mongo Sparkè¿æ¥å™¨ã€‚

ç›®å‰æœ‰3ä¸ªè¿æ¥å™¨å¯ç”¨ï¼ŒåŒ…æ‹¬ç¤¾åŒºç¬¬ä¸‰æ–¹å¼€å‘çš„å’Œä¹‹å‰Mongo Hadoopè¿æ¥å™¨ç­‰ï¼Œ

è¿™ä¸ªMong Sparkæ˜¯æœ€æ–°çš„ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬æ¨èçš„è¿æ¥æ–¹æ¡ˆã€‚

è¿™ä¸ªè¿æ¥å™¨æ˜¯ä¸“é—¨ä¸ºSparkæ‰“é€ çš„ï¼Œæ”¯æŒ**åŒå‘æ•°æ®**ï¼Œè¯»å‡ºå’Œå†™å…¥ã€‚

ä½†æ˜¯æœ€å…³é”®çš„æ˜¯ **æ¡ä»¶ä¸‹æ¨**ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š

å¦‚æœä½ åœ¨**Sparkç«¯**æŒ‡å®šäº†æŸ¥è¯¢æˆ–è€…é™åˆ¶æ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ª**æ¡ä»¶ä¼šè¢«ä¸‹æ¨åˆ°MongoDB**å»æ‰§è¡Œï¼Œ

è¿™æ ·å¯ä»¥ä¿è¯ä»MongoDBå–å‡ºæ¥ã€ç»è¿‡ç½‘ç»œä¼ è¾“åˆ°Sparkè®¡ç®—èŠ‚ç‚¹çš„æ•°æ®ç¡®å®éƒ½æ˜¯ç”¨å¾—ç€çš„ã€‚

æ²¡æœ‰**ä¸‹æ¨æ”¯æŒ**çš„è¯ï¼Œæ¯æ¬¡æ“ä½œå¾ˆå¯èƒ½**éœ€è¦ä»MongoDBè¯»å–å…¨é‡çš„æ•°æ®ï¼Œæ€§èƒ½ä½“éªŒå°†ä¼šå¾ˆç³Ÿç³•**ã€‚

æ‹¿åˆšæ‰çš„æ—¥å¿—ä¾‹å­æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬åªæƒ³å¯¹**404é”™è¯¯æ—¥å¿—**è¿›è¡Œåˆ†æï¼Œçœ‹é‚£äº›é”™è¯¯éƒ½æ˜¯å“ªäº›é¡µé¢ï¼Œä»¥åŠæ¯å¤©é”™è¯¯é¡µé¢æ•°é‡çš„å˜åŒ–ï¼Œ

å¦‚æœæœ‰æ¡ä»¶ä¸‹æ¨ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ç»™**MongoDBä¸€ä¸ªé™å®šæ¡ä»¶ï¼šé”™è¯¯ä»£ç =404**ï¼Œ

è¿™ä¸ªæ¡ä»¶ä¼šåœ¨MongoDBæœåŠ¡å™¨ç«¯æ‰§è¡Œï¼Œ

è¿™æ ·æˆ‘ä»¬åªéœ€è¦é€šè¿‡**ç½‘ç»œä¼ è¾“å¯èƒ½åªæ˜¯å…¨éƒ¨æ—¥å¿—çš„0.1%çš„æ•°æ®**ï¼Œè€Œä¸æ˜¯æ²¡æœ‰**æ¡ä»¶ä¸‹æ¨**æƒ…å†µä¸‹çš„å…¨éƒ¨æ•°æ®ã€‚

å¦å¤–ï¼Œè¿™ä¸ªæœ€æ–°çš„è¿æ¥å™¨è¿˜æ”¯æŒå’ŒSparkè®¡ç®—èŠ‚ç‚¹**Co-Lo éƒ¨ç½²**ã€‚

å°±æ˜¯è¯´åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸ŠåŒæ—¶éƒ¨ç½²**Sparkå®ä¾‹**å’Œ**MongoDBå®ä¾‹**ã€‚

è¿™æ ·åšå¯ä»¥å‡å°‘æ•°æ®åœ¨ç½‘ç»œä¸Šçš„ä¼ è¾“å¸¦æ¥çš„èµ„æºæ¶ˆè€—åŠæ—¶å»¶ã€‚

å½“ç„¶ï¼Œè¿™ç§éƒ¨ç½²æ–¹å¼éœ€è¦æ³¨æ„**å†…å­˜èµ„æºå’ŒCPUèµ„æº**çš„éš”ç¦»ã€‚éš”ç¦»çš„æ–¹å¼å¯ä»¥é€šè¿‡Linuxçš„**cgroups**ã€‚

#### <a name='-1'></a>2.12.1.1. æ¡ˆä¾‹

1. æ³•å›½èˆªç©ºæ˜¯æ³•å›½æœ€å¤§çš„èˆªç©ºå…¬å¸ï¼š

ä¸ºäº†æé«˜å®¢æˆ·ä½“éªŒï¼Œåœ¨æœ€è¿‘æ–½è¡Œçš„**360åº¦å®¢æˆ·è§†å›¾**ä¸­ï¼Œä½¿ç”¨Sparkå¯¹**å·²ç»æ”¶é›†åœ¨MongoDBé‡Œé¢çš„å®¢æˆ·æ•°æ®**è¿›è¡Œåˆ†ç±»åŠè¡Œä¸ºåˆ†æï¼Œå¹¶æŠŠç»“æœï¼ˆå¦‚å®¢æˆ·çš„ç±»åˆ«ã€æ ‡ç­¾ç­‰ä¿¡æ¯ï¼‰**å†™å›åˆ°MongoDBå†…æ¯ä¸€ä¸ªå®¢æˆ·çš„æ–‡æ¡£ç»“æ„**é‡Œã€‚

2. Stratioæ˜¯ç¾å›½ç¡…è°·ä¸€å®¶è‘—åçš„é‡‘èå¤§æ•°æ®å…¬å¸ï¼š

ä»–ä»¬æœ€è¿‘åœ¨ä¸€å®¶åœ¨**31ä¸ªå›½å®¶æœ‰åˆ†æ”¯æœºæ„çš„è·¨å›½é“¶è¡Œ**å®æ–½äº†ä¸€ä¸ª**å®æ—¶ç›‘æ§å¹³å°**ã€‚è¯¥é“¶è¡Œå¸Œæœ›é€šè¿‡**å¯¹æ—¥å¿—çš„ç›‘æ§å’Œåˆ†æ**æ¥ä¿è¯å®¢æˆ·æœåŠ¡çš„å“åº”æ—¶é—´ä»¥åŠ**å®æ—¶ç›‘æµ‹ä¸€äº›å¯èƒ½çš„è¿è§„æˆ–è€…é‡‘èæ¬ºè¯ˆè¡Œä¸º**ã€‚åœ¨è¿™ä¸ªåº”ç”¨å†…ï¼Œ ä»–ä»¬ä½¿ç”¨äº†ï¼š

* Apache Flume æ¥**æ”¶é›†log**

* Sparkæ¥**å¤„ç†å®æ—¶çš„log**

* MongoDBæ¥**å­˜å‚¨æ”¶é›†çš„log**ä»¥åŠ**Sparkåˆ†æçš„ç»“æœ**ï¼Œå¦‚**Key Performance Indicators**ç­‰

3. ä¸œæ–¹èˆªç©ºçš„æŒ‘æˆ˜ï¼š

é¡¾å®¢åœ¨ç½‘ç«™ä¸Šè®¢è´­æœºç¥¨ï¼Œå¹³å‡èµ„æ–™åº“æŸ¥è¯¢200æ¬¡å°±ä¼šä¸‹å•è®¢è´­æœºç¥¨ï¼Œä½†æ˜¯ç°åœ¨å¹³å‡è¦æŸ¥è¯¢1.2ä¸‡æ¬¡æ‰ä¼šå‘ç”Ÿä¸€æ¬¡è®¢è´­è¡Œä¸ºï¼Œ

**åŒæ ·çš„è®¢å•é‡ï¼ŒæŸ¥è¯¢é‡å´æˆé•¿ç™¾å€ã€‚**

æŒ‰ç…§50%ç›´é”€ç‡è¿™ä¸ªç›®æ ‡è®¡ç®—ï¼Œä¸œèˆªçš„è¿ä»·ç³»ç»Ÿè¦æ”¯æŒæ¯å¤©16äº¿çš„è¿ä»·è¯·æ±‚ã€‚

æ€è·¯ï¼šç©ºé—´æ¢æ—¶é—´

å½“å‰çš„è¿ä»·æ˜¯é€šè¿‡**å®æ—¶è®¡ç®—**çš„ï¼Œ

æŒ‰ç…§ç°åœ¨çš„è®¡ç®—èƒ½åŠ›ï¼Œéœ€è¦å¯¹å·²æœ‰ç³»ç»Ÿè¿›è¡Œ100å¤šå€çš„æ‰©å®¹ã€‚

å¦ä¸€ä¸ªå¸¸ç”¨çš„æ€è·¯ï¼Œå°±æ˜¯é‡‡ç”¨**ç©ºé—´æ¢æ—¶é—´**çš„æ–¹å¼ã€‚

ä¸å…¶å¯¹æ¯ä¸€æ¬¡çš„è¿ä»·è¯·æ±‚è¿›è¡Œè€—æ—¶300msçš„è¿ç®—ï¼Œä¸å¦‚äº‹å…ˆ**æŠŠæ‰€æœ‰å¯èƒ½çš„ç¥¨ä»·æŸ¥è¯¢ç»„åˆç©·ä¸¾å‡ºæ¥å¹¶è¿›è¡Œæ‰¹é‡è®¡ç®—**ï¼Œç„¶åæŠŠç»“æœ**å­˜å…¥MongoDB**é‡Œé¢ã€‚

å½“éœ€è¦**æŸ¥è¯¢è¿ä»·**æ—¶ï¼Œç›´æ¥æŒ‰ç…§ **å‡ºå‘+ç›®çš„åœ°+æ—¥æœŸçš„æ–¹å¼** åšä¸€ä¸ªå¿«é€Ÿçš„DBæŸ¥è¯¢ï¼Œå“åº”æ—¶é—´åº”è¯¥å¯ä»¥åšåˆ°å‡ åæ¯«ç§’ã€‚

é‚£ä¸ºä»€ä¹ˆè¦ç”¨MongoDBï¼Ÿå› ä¸ºæˆ‘ä»¬è¦å¤„ç†çš„æ•°æ®é‡åºå¤§æ— æ¯”ã€‚æŒ‰ç…§1000å¤šä¸ªèˆªç­ï¼Œ365å¤©ï¼Œ26ä¸ªä»“ä½ï¼Œ100å¤šæ¸ é“ä»¥åŠæ•°ä¸ªä¸åŒçš„èˆªç¨‹ç±»å‹ï¼Œ

æˆ‘ä»¬è¦å®æ—¶å­˜å–çš„è¿ä»·è®°å½•æœ‰æ•°åäº¿æ¡ä¹‹å¤šã€‚è¿™ä¸ªå·²ç»è¿œè¿œè¶…å‡º**å¸¸è§„RDBMS**å¯ä»¥æ‰¿å—çš„èŒƒå›´ã€‚

MongoDBåŸºäº**å†…å­˜ç¼“å­˜çš„æ•°æ®ç®¡ç†æ–¹å¼**å†³å®šäº†å¯¹**å¹¶å‘è¯»å†™çš„å“åº”**å¯ä»¥åšåˆ°**å¾ˆä½å»¶è¿Ÿ**ï¼Œ

**æ°´å¹³æ‰©å±•**çš„æ–¹å¼å¯ä»¥é€šè¿‡**å¤šå°èŠ‚ç‚¹åŒæ—¶å¹¶å‘å¤„ç†æµ·é‡è¯·æ±‚**ã€‚

äº‹å®ä¸Šï¼Œå…¨çƒæœ€å¤§çš„èˆªç©ºåˆ†é”€å•†ï¼Œç®¡ç†è€…å…¨ä¸–ç•Œ95%èˆªç©ºåº“å­˜çš„Amadeusä¹Ÿæ­£æ˜¯ä½¿ç”¨MongoDBä½œä¸ºå…¶1000å¤šäº¿**è¿ä»·ç¼“å­˜çš„å­˜å‚¨æ–¹æ¡ˆ**ã€‚

#### <a name='-1'></a>2.12.1.2. è¿ä»·ç³»ç»Ÿçš„æ¶æ„å›¾

å·¦è¾¹æ˜¯å‘èµ·èˆªç­æŸ¥è¯¢è¯·æ±‚çš„å®¢æˆ·ç«¯ï¼Œ

é¦–å…ˆä¼šæœ‰**APIæœåŠ¡å™¨**è¿›è¡Œ**é¢„å¤„ç†**ï¼šä¸€èˆ¬èˆªç­è¯·æ±‚ä¼šåˆ†ä¸º**åº“å­˜æŸ¥è¯¢**å’Œ**è¿ä»·æŸ¥è¯¢**ã€‚**åº“å­˜æŸ¥è¯¢**ä¼šç›´æ¥åˆ°ä¸œèˆªå·²æœ‰çš„**åº“å­˜ç³»ç»Ÿï¼ˆSeat Inventoryï¼‰**ï¼ŒåŒæ ·æ˜¯å®ç°åœ¨MongoDBä¸Šé¢çš„ã€‚åœ¨ç¡®å®šåº“å­˜åæ ¹æ®**åº“å­˜ç»“æœ**å†ä»**Fare Cacheç³»ç»Ÿ**å†…æŸ¥è¯¢ç›¸åº”çš„è¿ä»·ã€‚

**Sparké›†ç¾¤**åˆ™æ˜¯å¦å¤–ä¸€å¥—è®¡ç®—é›†ç¾¤ï¼Œé€šè¿‡**Spark MongoDBè¿æ¥å¥—ä»¶**å’Œ**MongoDB Fare Cacheé›†ç¾¤**è¿æ¥ã€‚

Spark è®¡ç®—ä»»åŠ¡ä¼š**å®šæœŸè§¦å‘ï¼ˆå¦‚æ¯å¤©ä¸€æ¬¡æˆ–è€…æ¯4å°æ—¶ä¸€æ¬¡ï¼‰**ï¼Œè¿™ä¸ªä»»åŠ¡ä¼šå¯¹æ‰€æœ‰çš„å¯èƒ½çš„**è¿ä»·ç»„åˆè¿›è¡Œå…¨é‡è®¡ç®—**ï¼Œç„¶åå­˜å…¥**MongoDB**ï¼Œä»¥ä¾›æŸ¥è¯¢ä½¿ç”¨ã€‚

å³åŠè¾¹åˆ™æŠŠåŸæ¥**å®æ—¶è¿ç®—çš„é›†ç¾¤æ¢æˆäº†Spark+MongoDB**ã€‚Sparkè´Ÿè´£**æ‰¹é‡è®¡ç®—ä¸€å¹´å†…æ‰€æœ‰èˆªç­æ‰€æœ‰ä»“ä½çš„æ‰€æœ‰ä»·æ ¼**ï¼Œå¹¶ä»¥**é«˜å¹¶å‘**çš„å½¢å¼å­˜å‚¨åˆ°MongoDBé‡Œé¢ã€‚

æ¯ç§’é’Ÿå¤„ç†çš„è¿ä»·å¯ä»¥è¾¾åˆ°æ•°ä¸‡æ¡ã€‚

å½“æ¥è‡ªå®¢æˆ·ç«¯çš„è¿ä»·æŸ¥è¯¢è¾¾åˆ°æœåŠ¡ç«¯ä»¥åï¼Œ**æœåŠ¡ç«¯**ç›´æ¥å°±å‘MongoDBå‘å‡ºæŒ‰ç…§**æ—¥æœŸ**ï¼Œ**å‡ºå‘**ï¼Œ**åˆ°è¾¾æœºåœº**ä¸ºæ¡ä»¶çš„mongoæŸ¥è¯¢ã€‚

![image](https://raw.githubusercontent.com/YutingYao/DailyJupyter/main/imageSever/image.6uytn281yhc0.png)

éœ€è¦è®¡ç®—çš„ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯æ‰€æœ‰**æ—¥æœŸã€èˆªç­ã€ä»“ä½**çš„ç»„åˆï¼Œäº‹å…ˆå·²ç»å­˜æ”¾åˆ°**MongoDB**é‡Œé¢ã€‚

ä»»åŠ¡é€’äº¤åˆ°**master**ï¼Œç„¶å**é¢„å…ˆåŠ è½½æ‰€éœ€å‚è€ƒæ•°æ®**ï¼Œ

**broadcast**å°±æ˜¯æŠŠè¿™äº›åœ¨å†…å­˜é‡Œçš„æ•°æ®å¤åˆ¶åˆ°æ¯ä¸€ä¸ª**Sparkè®¡ç®—èŠ‚ç‚¹çš„JVM**ï¼Œ

ç„¶åæ‰€æœ‰è®¡ç®—èŠ‚ç‚¹**å¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œ**ï¼Œ

ä»Mongodbé‡Œå–å‡ºéœ€è¦è®¡ç®—çš„ä»“ä½ï¼Œè°ƒç”¨ä¸œèˆªè‡ªå·±çš„**è¿ä»·é€»è¾‘**ï¼Œå¾—å‡ºç»“æœä»¥åï¼Œå¹¶ä¿å­˜å›MongoDBã€‚

### <a name='Spark-1'></a>2.12.2. Spark ä»»åŠ¡å…¥å£ç¨‹åº

Sparkå’ŒMongoDBçš„è¿æ¥ä½¿ç”¨éå¸¸ç®€å•ï¼Œä¸‹é¢å°±æ˜¯ä¸€ä¸ªä»£ç ç¤ºä¾‹ï¼š

```java
// initialization dependencies including base prices, pricing rules and some reference data
Map dependencies = MyDependencyManager.loadDependencies();
// broadcasting dependencies
javaSparkContext.broadcast(dependencies);

// create job rdd
cabinsRDD = MongoSpark.load(javaSparkContext).withPipeline(pipeline)

// for each cabin, date, airport pair, calculate the price
cabinsRDD.map(function calc_price);

// collect the result, which will cause the data to be stored into MongoDB
cabinsRDD.collect()
cabinsRDD.saveToMongo()
```

### <a name='SparkMongoDB'></a>2.12.3. Spark ï¼‹ MongoDBæ¼”ç¤º

å®‰è£… Sparkï¼ˆç•¥ï¼‰

æµ‹è¯•è¿æ¥å™¨

```sql
# cd ï½ï¼spark
# ./bin/spark-shell \
--conf "spark.mongodb.input.uri=mongodb://127.0.0.1/flights.av" \
--conf "spark.mongodb.output.uri=mongodb://127.0.0.1/flights.output" \
--packages org.mongodb.spark:mongo-spark-connector_2.10:1.0.0

import com.mongodb.spark._
import org.bson.Document

MongoSpark.load(sc).take(10).foreach(println)
```

ç®€å•åˆ†ç»„ç»Ÿè®¡
æ•°æ®ï¼š 365å¤©ï¼Œæ‰€æœ‰èˆªç­åº“å­˜ä¿¡æ¯ï¼Œ500ä¸‡æ–‡æ¡£
ä»»åŠ¡ï¼š æŒ‰èˆªç­ç»Ÿè®¡ä¸€å¹´å†…æ‰€æœ‰ä½™ç¥¨é‡

```sql
MongoSpark.load(sc)
     .map(doc=>(doc.getString("flight") ,doc.getLong("seats")))
     .reduceByKey((x,y)=>(x+y))
      .take(10)
     .foreach(println)
```

ç®€å•åˆ†ç»„ç»Ÿè®¡åŠ æ¡ä»¶è¿‡æ»¤
æ•°æ®ï¼š 365å¤©ï¼Œæ‰€æœ‰èˆªç­åº“å­˜ä¿¡æ¯ï¼Œ500ä¸‡æ–‡æ¡£
ä»»åŠ¡ï¼š æŒ‰èˆªç­ç»Ÿè®¡ä¸€å¹´å†…æ‰€æœ‰åº“å­˜ï¼Œä½†æ˜¯åªå¤„ç†æ˜†æ˜å‡ºå‘çš„èˆªç­

```sql
import org.bson.Document

MongoSpark.load(sc)
          .withPipeline(Seq(Document.parse("{ $match: { orig :  'KMG'  } }")))
    .map(doc=>(doc.getString("flight") ,doc.getLong("seats")))
    .reduceByKey((x,y)=>(x+y))
    .take(10)
    .foreach(println)
```

æ€§èƒ½ä¼˜åŒ–äº‹é¡¹:

* ä½¿ç”¨åˆé€‚çš„**chunksize (MB)**
* Total data size / chunksize = chunks = RDD partitions = spark tasks
* ä¸è¦å°†**æ‰€æœ‰CPUæ ¸**åˆ†é…ç»™Spark
* é¢„ç•™**1-2ä¸ªcore**ç»™**æ“ä½œç³»ç»Ÿ**åŠ**å…¶ä»–ç®¡ç†è¿›ç¨‹**
* åŒæœºéƒ¨ç½²ï¼Œé€‚å½“æƒ…å†µå¯ä»¥**åŒæœºéƒ¨ç½²Spark+MongoDB**ï¼Œåˆ©ç”¨**æœ¬åœ°IO**æé«˜æ€§èƒ½

## <a name='SBT'></a>2.13. å®‰è£…SBT

### <a name='LinuxSBT'></a>2.13.1. Linuxä¸­å®‰è£…SBT

1. åœ¨å®˜ç½‘ä¸Šä¸‹è½½.tgzå®‰è£…åŒ…

<https://www.scala-sbt.org/download.html>

2. ä½¿ç”¨tar -zxvf å¯¹å®‰è£…åŒ…è¿›è¡Œè§£å‹

```sh
tar -zxvf sbt-1.3.2.tgz 
```


##  Sparkå¿«é€Ÿå…¥é—¨ä¹‹SBTå®‰è£…

Sparkä¸­æ²¡æœ‰è‡ªå¸¦sbtï¼Œéœ€è¦æ‰‹åŠ¨å®‰è£…sbtï¼Œ

æˆ‘çš„æ–¹æ³•æ˜¯ä¸‹è½½`sbt-launch.jar`ï¼Œç„¶åå°†æºæ”¹ä¸ºå›½å†…æºï¼ˆaliyunï¼‰ï¼Œ

æˆ‘é€‰æ‹©å°†sbtå®‰è£…åœ¨`/opt/sbt`ä¸­ã€‚

```sh
sudo mkdir /opt/sbt
sudo chown -R hadoop /opt/sbt   #username is hadoop.
cd /opt/sbt
mkdir sbtlaunch   #store sbt-launch.jar
```

1. ä¸‹è½½sbt-launch.jar,å¹¶å­˜æ”¾è‡³/opt/sbt/sbtlaunch

```sh
cd /opt/sbt/sbtlaunch
wget https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.9/sbt-launch.jar -O ./sbt-launch.jar   #download sbt-launch.jar
unzip -q ./sbt-launch.jar #è§£å‹
$ 
```

# 3. zeppelinå¸¸ç”¨å‘½ä»¤

```sh
sudo vim conf/zeppelin-site.xml
bin/zeppelin-daemon.sh restart
vim logs/zeppelin-xxxxx-Pro.local.log
```

## <a name='local'></a>3.1. åœ¨localæ¨¡å¼ä¸‹è¿è¡Œ

```sh
tar -xvf flink-1.10.0-bin-scala_2.11.tgz
```

**minicluster** çš„ç«¯å£ä¸º **8081**

æŸ¥çœ‹log

```sh
cd ../zeppelin-0.9.0-SNAPSHOT
vim logs/zeppelin-è‡ªåŠ¨è¡¥å…¨ï¼Ÿ
```

tabé”®è‡ªåŠ¨è¡¥å…¨å‘½ä»¤

## <a name='remote'></a>3.2. åœ¨remoteæ¨¡å¼ä¸‹è¿è¡Œ

flink.excution.modeè®¾ç½®ä¸ºremote
flink.excution.remote.hostè®¾ç½®ä¸ºlocalhost
flink.excution.remote.portè®¾ç½®ä¸º**8081**

## <a name='yarn'></a>3.3. yarnæ¨¡å¼ä¸‹çš„è¿è¡Œ

ç¡®ä¿hadoopå·²ç»å®‰è£…

```sh
hadoop classpath
```

è·å¾—hadoopçš„æ‰€æœ‰jar

```sh
echo $HADOOP_CONF_DIR
```

* FLINK_HOMEè®¾ç½®ä¸º/Users/xxx/xxx/flink-1.10.0
* flink.excution.modeè®¾ç½®ä¸ºyarn
* flink.excution.remote.hostè®¾ç½®ä¸ºlocalhost
* flink.excution.remote.portè®¾ç½®ä¸º8081
* flink.jm.memoryè®¾ç½®ä¸º1024
* flink.tm.memoryè®¾ç½®ä¸º1024
* flink.tm.slotè®¾ç½®ä¸º2
* local.number-taskmanagerè®¾ç½®ä¸º4
* flink.yarn.appNameè®¾ç½®ä¸ºZeppelin Flink Session
* flink.yarn.queueè®¾ç½®ä¸ºdefault
* zeppelin.flink.maxResultè®¾ç½®ä¸º1000
* zeppelin.pyflink.pythonè®¾ç½®ä¸º/Users/xxx/anaconda3/bin/python

```sh
ps aux | grep RemoteInterpreterServer
```

flinkçš„classpath

## <a name='inlineconfiguration'></a>3.4. inline configuration

ä¸€å®šè¦åœ¨è¿›ç¨‹èµ·æ¥å‰è·‘

```sql
%flink.conf
flink.execution.mode yarn
```

## <a name='hive'></a>3.5. hive

å¸¸ç”¨å‘½ä»¤ï¼š

```sh
bin/hive # å¼€å¯è¿›ç¨‹
```

```sh
show tables 
```

```sh
quit # é€€å‡º
```

å…ˆè¦copyä¸€äº›jar(ä¸åŒç‰ˆæœ¬ï¼Œè¦copyçš„jarä¸åŒ)ï¼š

```sh
cp ~/flink-connector-hive-2.11-1.10.0.jar ~/Flink_Videos/flink-1.10.0/lin
cp lib/hive-exec-2.3.4.jar  ~/Flink_Videos/flink-1.10.0/lin
```

```sh
cd conf
pwd
```

æŠŠç›®å½•copyä¸‹æ¥ï¼Œæ”¾åˆ°é…ç½®é¡µé¢

```sql
%flink.bsql
show tables;
select * from bank;
```

## <a name='SQL'></a>3.6. SQL

```sql
%flink.bsql
show tables;
--this is a comment
showfunctions
```

```sql
%flink.ssql
show tables;
--this is a comment
showfunctions
```

## <a name='Streaming'></a>3.7. Streaming

é‡‡ç”¨Flink Job Control Tutorialè¿›è¡Œå­¦ä¹ ï¼š

singleæ¨¡å¼ä¸‹ï¼šæŒ‡selectè¯­å¥åªæœ‰ä¸€è¡Œ

è¿™é‡Œå¿…é¡»ç”¨åˆ°htmlçš„æ¨¡æ¿ï¼Œ

{0}æŒ‡ä»£max(rowtime)

{1}æŒ‡ä»£count(*)

é»˜è®¤çš„åˆ·æ–°é¢‘ç‡æ˜¯3ç§’

```sql
% flink.ssql(type = single, refreshInterval = 1000, template = Total count is <h1>{1}</h1> <br> {0})

select max(rowtime), count(*) from log
```

updateæ¨¡å¼ä¸‹ï¼šæ¯ä¸€æ¬¡æ›´æ–°æ•°æ®ï¼Œéƒ½æ˜¯å¯¹åŸæ¥çš„æ•°æ®åšä¸€æ¬¡update

é»˜è®¤æ˜¯tableæ¨¡å¼ï¼Œä¸éœ€è¦åˆ¶å®štemplate

```sql
%flink.ssql(type)

select url, count(1) as c from log group by url
```

appendæ¨¡å¼ä¸‹ï¼šä¼šå¾—åˆ°æ—¶é—´åºåˆ—æ—¶é—´ã€‚ç¬¬ä¸€ä¸ªå­—æ®µï¼Œselectå­—æ®µï¼Œå¿…é¡»æ˜¯æ—¶é—´ã€‚

settingsçš„è®¾ç½®ï¼š

keysï¼šæ°¸è¿œè®¾ç½®ä¸ºæ—¶é—´

valuesï¼šè®¾ç½®ä¸ºPVå€¼

groupsï¼šè®¾ç½®ä¸ºURL(home search product)

threshold: é»˜è®¤ä¿ç•™ä¸€ä¸ªå°æ—¶çš„æ•°æ®ï¼Œä½†ä¹Ÿå¯ä»¥è®¾ç½®ä¸º60000ï¼Œè¡¨ç¤ºä¿ç•™1åˆ†é’Ÿçš„æ•°æ®

ä»¥5ç§’ä¸ºä¸€ä¸ªçª—å£çš„å•å…ƒï¼ŒæŸ¥çœ‹5ç§’ä»¥å†…ï¼Œæ¯ä¸€ä¸ªçª—å£çš„pvï¼š

```sql
%flink.ssql(type = append, threshold)
select TUMBEL_START(rowtime, INTERVAL '5' SECOND) as
start_time, url, count(1) as pv from log group by
TUMBLE(rowtime, INTERVAL '5' SECOND), url
```

## <a name='kafka-1'></a>3.8. kafka

## <a name='python'></a>3.9. python

## <a name='spark-1'></a>3.10. spark

é¦–å…ˆç¡®è®¤Zeppelinçš„æœºå™¨ä¸Šå·²å®‰è£…æœ‰Hadoopå®¢æˆ·ç«¯å’ŒSparkå®¢æˆ·ç«¯ï¼Œ

èƒ½é€šè¿‡Hadoopå®¢æˆ·ç«¯è¿æ¥HDFSï¼Œ

é€šè¿‡Sparkå®¢æˆ·ç«¯æäº¤ä»»åŠ¡ç»™YARNã€‚

```sh
cd zeppelin-0.9.0-bin-all
vi conf/zeppelin-env.sh

# åœ¨zeppelin-env.shæ–‡ä»¶ä¸­æ‰¾åˆ°SPARK_HOMEå’ŒHADOOP_CONF_DIRä¸¤é¡¹é…ç½®ï¼Œä¿®æ”¹æˆå®é™…çš„è·¯å¾„
export SPARK_HOME=/opt/cloudera/parcels/CDH/lib/spark
export HADOOP_CONF_DIR=/etc/hadoop/conf
# é‡å¯Zeppelin
./bin/zeppelin-daemon.sh restart
```

é€‰æ‹©Interpreter

æœç´¢spark

å°†spark.masteré…ç½®æˆyarn-clientï¼Œå…¶ä»–å¯ä»¥æš‚æ—¶ä¿æŒä¸å˜ã€‚

éªŒè¯æµ‹è¯•

```sql
%spark
import org.apache.hadoop.fs.{FileSystem, Path}
val fs = FileSystem.get(sc.hadoopConfiguration)
val dirSize = fs.getContentSummary(new Path("hdfs:///user/root")).getLength
```

## <a name='flink-Pythonenv-Conda'></a>3.11. flink - Python env - Conda

### <a name='-1'></a>3.11.1. å‡†å¤‡å·¥ä½œ

æœ¬æ–‡å†…å®¹å°±æ˜¯åœ¨ Zeppelin notebook é‡Œåˆ©ç”¨ Conda æ¥åˆ›å»º Python env è‡ªåŠ¨éƒ¨ç½²åˆ° Yarn é›†ç¾¤ä¸­ï¼Œæ— éœ€æ‰‹åŠ¨åœ¨é›†ç¾¤ä¸Šå»å®‰è£…ä»»ä½• Pyflink çš„åŒ…ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸€ä¸ª Yarn é›†ç¾¤é‡ŒåŒæ—¶ä½¿ç”¨å¤šä¸ªç‰ˆæœ¬çš„ PyFlinkã€‚

ä¸‹è½½ Flink 1.13ï¼Œ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ¬æ–‡çš„åŠŸèƒ½åªèƒ½ç”¨åœ¨ Flink 1.13 ä»¥ä¸Šç‰ˆæœ¬ï¼Œ

ç„¶åï¼š

æŠŠ **flink-Python-*.jar** è¿™ä¸ª jar åŒ… copy åˆ° **Flink çš„ lib æ–‡ä»¶å¤¹**ä¸‹ï¼›

æŠŠ **opt/Python** è¿™ä¸ªæ–‡ä»¶å¤¹ copy åˆ° **Flink çš„ lib æ–‡ä»¶å¤¹**ä¸‹ã€‚

å®‰è£…ä»¥ä¸‹è½¯ä»¶ (è¿™äº›è½¯ä»¶æ˜¯ç”¨äºåˆ›å»º Conda env çš„)ï¼š

* [miniconda](https://docs.conda.io/en/latest/miniconda.html)

* [conda pack](https://conda.github.io/conda-pack/)

* [mamba](https://github.com/mamba-org/mamba)

### <a name='PyFlink'></a>3.11.2. æ­å»º PyFlink ç¯å¢ƒ

æ¥ä¸‹æ¥å°±å¯ä»¥åœ¨ Zeppelin é‡Œæ­å»ºå¹¶ä¸”ä½¿ç”¨ PyFlink äº†ã€‚

#### <a name='Step1.JobManagerPyFlinkConda'></a>3.11.2.1. Step 1. åˆ¶ä½œ **JobManager** ä¸Šçš„ **PyFlink Conda** ç¯å¢ƒ

å› ä¸º Zeppelin å¤©ç”Ÿæ”¯æŒ Shellï¼Œ

æ‰€ä»¥å¯ä»¥åœ¨ Zeppelin é‡Œç”¨ Shell æ¥åˆ¶ä½œ PyFlink ç¯å¢ƒã€‚

æ³¨æ„è¿™é‡Œçš„ **Python ç¬¬ä¸‰æ–¹åŒ…**æ˜¯åœ¨ **PyFlink å®¢æˆ·ç«¯ (JobManager)** éœ€è¦çš„åŒ…ï¼Œ

æ¯”å¦‚ Matplotlib è¿™äº›ï¼Œå¹¶ä¸”ç¡®ä¿è‡³å°‘å®‰è£…äº†ä¸‹é¢è¿™äº›åŒ…ï¼š

æŸä¸ªç‰ˆæœ¬çš„ **Python (è¿™é‡Œç”¨çš„æ˜¯ 3.7ï¼‰**

**apache-flink (è¿™é‡Œç”¨çš„æ˜¯ 1.13.1)**

**jupyterï¼Œgrpcioï¼Œprotobuf** (è¿™ä¸‰ä¸ªåŒ…æ˜¯ Zeppelin éœ€è¦çš„)

å‰©ä¸‹çš„åŒ…å¯ä»¥æ ¹æ®éœ€è¦æ¥æŒ‡å®šï¼š

```sql
%sh

# make sure you have conda and momba installed.
# install miniconda: https://docs.conda.io/en/latest/miniconda.html
# install mamba: https://github.com/mamba-org/mamba

echo "name: pyflink_env
channels:
- conda-forge
- defaults
dependencies:
- Python=3.7
- pip
- pip:
  - apache-flink==1.13.1
- jupyter
- grpcio
- protobuf
- matplotlib
- pandasql
- pandas
- scipy
- seaborn
- plotnine
" > pyflink_env.yml
   
mamba env remove -n pyflink_env
mamba env create -f pyflink_env.yml
```

è¿è¡Œä¸‹é¢çš„ä»£ç æ‰“åŒ… PyFlink çš„ **Conda ç¯å¢ƒ**å¹¶ä¸”**ä¸Šä¼ **åˆ° **HDFS** (æ³¨æ„è¿™é‡Œæ‰“åŒ…å‡ºæ¥çš„æ–‡ä»¶æ ¼å¼æ˜¯ tar.gz)ï¼š

```sql
%sh

rm -rf pyflink_env.tar.gz
conda pack --ignore-missing-files -n pyflink_env -o pyflink_env.tar.gz

hadoop fs -rmr /tmp/pyflink_env.tar.gz
hadoop fs -put pyflink_env.tar.gz /tmp
# The Python conda tar should be public accessible, so need to change permission here.
hadoop fs -chmod 644 /tmp/pyflink_env.tar.gz
```

#### <a name='Step2.TaskManagerPyFlinkConda'></a>3.11.2.2. Step 2. åˆ¶ä½œ TaskManager ä¸Šçš„ PyFlink Conda ç¯å¢ƒ

è¿è¡Œä¸‹é¢çš„ä»£ç æ¥åˆ›å»º **TaskManager ä¸Šçš„ PyFlink Conda ç¯å¢ƒ**ï¼Œ

TaskManager ä¸Šçš„ PyFlink ç¯å¢ƒ**è‡³å°‘åŒ…å«ä»¥ä¸‹ 2 ä¸ªåŒ…**ï¼š

* æŸä¸ªç‰ˆæœ¬çš„ Python (è¿™é‡Œç”¨çš„æ˜¯ 3.7ï¼‰

* apache-flink (è¿™é‡Œç”¨çš„æ˜¯ 1.13.1)

å‰©ä¸‹çš„åŒ…æ˜¯ **Python UDF** éœ€è¦ä¾èµ–çš„åŒ…ï¼Œæ¯”å¦‚è¿™é‡ŒæŒ‡å®šäº† **pandas**

```sql
%sh

echo "name: pyflink_tm_env
channels:
- conda-forge
- defaults
dependencies:
- Python=3.7
- pip
- pip:
  - apache-flink==1.13.1
- pandas
" > pyflink_tm_env.yml
   
mamba env remove -n pyflink_tm_env
mamba env create -f pyflink_tm_env.yml
```

è¿è¡Œä¸‹é¢çš„ä»£ç æ‰“åŒ… PyFlink çš„ Conda ç¯å¢ƒå¹¶ä¸”ä¸Šä¼ åˆ° HDFS (æ³¨æ„è¿™é‡Œä½¿ç”¨çš„æ˜¯ zip æ ¼å¼ï¼‰ï¼š

```sql
%sh

rm -rf pyflink_tm_env.zip
conda pack --ignore-missing-files --zip-symlinks -n pyflink_tm_env -o pyflink_tm_env.zip

hadoop fs -rmr /tmp/pyflink_tm_env.zip
hadoop fs -put pyflink_tm_env.zip /tmp
# The Python conda tar should be public accessible, so need to change permission here.
hadoop fs -chmod 644 /tmp/pyflink_tm_env.zip
```

#### <a name='Step3.PyFlinkConda'></a>3.11.2.3. Step 3. åœ¨ PyFlink ä¸­ä½¿ç”¨ Conda ç¯å¢ƒ

æ¥ä¸‹æ¥å°±å¯ä»¥åœ¨ Zeppelin ä¸­ä½¿ç”¨ä¸Šé¢åˆ›å»ºçš„ Conda ç¯å¢ƒäº†ï¼Œ

é¦–å…ˆéœ€è¦åœ¨ Zeppelin é‡Œé…ç½® Flinkï¼Œä¸»è¦**é…ç½®çš„é€‰é¡¹**æœ‰ï¼š

* **flink.execution.mode** ä¸º **yarn-application**, æœ¬æ–‡æ‰€è®²çš„æ–¹æ³•åªé€‚ç”¨äº **yarn-application æ¨¡å¼**ï¼›

* æŒ‡å®š **yarn.ship-archives**ï¼Œ**zeppelin.pyflink.Python** ä»¥åŠ **zeppelin.interpreter.conda.env.name** æ¥é…ç½® **JobManager** ä¾§çš„ **PyFlink Conda ç¯å¢ƒ**ï¼›

* æŒ‡å®š **Python.archives** ä»¥åŠ **Python.executable** æ¥æŒ‡å®š **TaskManager** ä¾§çš„ **PyFlink Conda ç¯å¢ƒ**ï¼›

* æŒ‡å®šå…¶ä»–**å¯é€‰çš„ Flink é…ç½®**ï¼Œæ¯”å¦‚è¿™é‡Œçš„ **flink.jm.memory** å’Œ **flink.tm.memory**ã€‚

```sql
%flink.conf


flink.execution.mode yarn-application

yarn.ship-archives /mnt/disk1/jzhang/zeppelin/pyflink_env.tar.gz
zeppelin.pyflink.Python pyflink_env.tar.gz/bin/Python
zeppelin.interpreter.conda.env.name pyflink_env.tar.gz

Python.archives hdfs:///tmp/pyflink_tm_env.zip
Python.executable pyflink_tm_env.zip/bin/Python3.7

flink.jm.memory 2048
flink.tm.memory 2048
```

æ¥ä¸‹æ¥å°±å¯ä»¥å¦‚ä¸€å¼€å§‹æ‰€è¯´çš„é‚£æ ·åœ¨ Zeppelin é‡Œä½¿ç”¨ **PyFlink ä»¥åŠæŒ‡å®šçš„ Conda ç¯å¢ƒ**äº†ã€‚æœ‰ 2 ç§åœºæ™¯:

ä¸‹é¢çš„ä¾‹å­é‡Œï¼Œå¯ä»¥åœ¨ PyFlink å®¢æˆ·ç«¯ (JobManager ä¾§)

ä½¿ç”¨ä¸Šé¢åˆ›å»ºçš„ JobManager ä¾§çš„ Conda ç¯å¢ƒï¼Œ

æ¯”å¦‚ä¸‹è¾¹ä½¿ç”¨äº† **Matplotlib**ã€‚

![image](https://raw.githubusercontent.com/YutingYao/DailyJupyter/main/imageSever/image.u24vlra1yzk.png)

ä¸‹é¢çš„ä¾‹å­æ˜¯åœ¨ **PyFlink UDF** é‡Œä½¿ç”¨ä¸Šé¢åˆ›å»ºçš„ **TaskManager ä¾§ Conda ç¯å¢ƒé‡Œçš„åº“**ï¼Œ

æ¯”å¦‚ä¸‹é¢åœ¨ UDF é‡Œä½¿ç”¨ **Pandas**ã€‚

![image](https://raw.githubusercontent.com/YutingYao/DailyJupyter/main/imageSever/image.5s0w557gjm80.png)

## <a name='ApacheSedona'></a>3.12. Apache Sedona

[Apache Sedona](https://github.com/apache/incubator-sedona)(å­µåŒ–)æ˜¯ä¸€ä¸ªç”¨äºå¤„ç†å¤§è§„æ¨¡ç©ºé—´æ•°æ®çš„é›†ç¾¤è®¡ç®—ç³»ç»Ÿã€‚

Sedonaé€šè¿‡ä¸€ç»„å¼€ç®±ä½¿ç”¨çš„ç©ºé—´å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†(srdd)/

SpatialSQLæ‰©å±•äº†Apache Spark / SparkSQLï¼Œ

å¯ä»¥æœ‰æ•ˆåœ°è·¨æœºå™¨åŠ è½½ã€å¤„ç†å’Œåˆ†æå¤§è§„æ¨¡ç©ºé—´æ•°æ®ã€‚

| Name  |  API |  ä»‹ç»|
|---|---|---|
|Core  | RDD  | SpatialRDD å’ŒæŸ¥è¯¢è¿ç®—ç¬¦ã€‚ |
|SQL  | SQL/DataFrame  |Sedona æ ¸å¿ƒçš„ SQL æ¥å£ã€‚|
|Viz |  RDD, SQL/DataFrame | ç©ºé—´ RDD å’Œ DataFrame çš„å¯è§†åŒ–|
|Zeppelin |  Apache Zeppelin | Apache Zeppelin 0.8.1+ æ’ä»¶|

è¿™æ˜¯ä¸€ä¸ª[jupyterç¤ºä¾‹](https://mybinder.org/v2/gh/apache/incubator-sedona/HEAD?filepath=binder)

å¯ä»¥å®‰è£…åœ¨[zeppelin](https://github.com/apache/incubator-sedona/tree/master/zeppelin)ä¸Š

## <a name='oracle'></a>3.13. oracle (è²Œä¼¼ä¸å¤ªå¸¸ç”¨)

## <a name='oracle-1'></a>3.14. ç®€å•ä»‹ç»oracle

Oracleæ•°æ®åº“ä¸­çš„ç©ºé—´å’Œå›¾å½¢ç‰¹æ€§

Oracleæ•°æ®åº“ç°åœ¨åŒ…æ‹¬**æœºå™¨å­¦ä¹ **ï¼Œ**ç©ºé—´**å’Œ**å›¾å½¢åŠŸèƒ½**ã€‚

å¦‚æœä½ æœ‰Oracleæ•°æ®åº“è®¸å¯è¯ï¼Œä½ å¯ä»¥ä½¿ç”¨æ‰€æœ‰è¡Œä¸šé¢†å…ˆçš„æœºå™¨å­¦ä¹ ã€ç©ºé—´å’Œå›¾å½¢åŠŸèƒ½ï¼Œ

åœ¨**æœ¬åœ°**å’Œ**Oracleäº‘æ•°æ®åº“**æœåŠ¡ä¸­è¿›è¡Œå¼€å‘å’Œéƒ¨ç½²

ä¸€äº›åº”ç”¨ï¼š

ä½¿ç”¨[oracle](https://www.jianshu.com/p/08afbdc63848/)ä½œä¸ºæ•°æ®æºå‘å¸ƒå›¾å±‚åˆ°[geoserver](https://docs.geoserver.org/latest/en/user/data/database/oracle.html)

### <a name='Oracle'></a>3.14.1. è¿æ¥Oracleæ•°æ®åº“

ç®€å•æ¥è¯´ï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š

1. ä¸‹è½½ojdbc8.jar
2. åˆ›å»ºæ–°jdbcè§£é‡Šå™¨
3. é…ç½®jdbcå‚æ•°
4. æµ‹è¯•æ–°è§£é‡Šå™¨

1. è¿›å…¥ Interpreters page.
2. åˆ›å»º new jdbc Interpreter.
3. é…ç½®å‚æ•°ã€‚

```sql
default.driver oracle.jdbc.driver.OracleDriver
default.url  jdbc:oracle:thin:@//host:port/servicename
default.user  database_user
default.password password
artifact   /opt/oracle/ojdbc8.jar
```

ç”¨æ–°çš„è§£é‡Šå™¨åˆ›å»ºæ–°çš„notbookç»‘å®šã€‚

```sql

```

```sql

```

```sql

```

# 4. é«˜é˜¶æŠ€å·§

[ä½¿ç”¨ Flink å‰éœ€è¦çŸ¥é“çš„ 10 ä¸ªã€é™·é˜±ã€](https://mp.weixin.qq.com/s/iQdYaChIftZckyXRy3tZ0g)

[æˆ‘å¸Kafka+Flink+MySQLç”Ÿäº§å®Œæ•´æ¡ˆä¾‹ä»£ç ](https://mp.weixin.qq.com/s/enbuh3BGp1ocAlCoSyQysQ)ï¼Œè¿™ä¸ªæ¡ˆä¾‹ç”¨çš„æ˜¯java

[ã€Flinkã€‘ç¬¬äºŒåå…­ç¯‡ï¼šæºç è§’åº¦åˆ†æTaskæ‰§è¡Œè¿‡ç¨‹](https://mp.weixin.qq.com/s/BOxSh3YltFrrT_IupQAB6Q)ï¼Œè¿™ä¸ªæ¡ˆä¾‹ç”¨çš„æ˜¯java

[å®æ—¶æ•°ä»“ | Flinkå®æ—¶ç»´è¡¨joinæ–¹æ³•æ€»ç»“ï¼ˆé™„é¡¹ç›®æºç ï¼‰](https://mp.weixin.qq.com/s/X3YYm9psakwF-HamjCvKBg)ï¼Œè¿™ä¸ªæ¡ˆä¾‹ç”¨çš„æ˜¯java
